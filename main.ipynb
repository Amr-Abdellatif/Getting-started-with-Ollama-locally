{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Ollama & Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Have two API's to be consider natively by Ollama : \n",
    "#### 1. Generate\n",
    "#### 2. chat\n",
    "Reference: \n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "text = \"Hello this is something about my home country, egypt \"\n",
    "\n",
    "response = requests.post('http://localhost:11434/api/generate', \n",
    "        json={\n",
    "            \"model\": \"llama3:8b\",\n",
    "            \"prompt\": f\"Summarize this article in 3 bullet points: {text}\",\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.3,\n",
    "            }}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:8b',\n",
       " 'created_at': '2024-11-25T17:40:16.0643624Z',\n",
       " 'response': \"I'm happy to help! However, I don't see an article attached or provided. Could you please share the article with me so I can summarize it in 3 bullet points for you?\",\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'context': [128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  9370,\n",
       "  5730,\n",
       "  553,\n",
       "  420,\n",
       "  4652,\n",
       "  304,\n",
       "  220,\n",
       "  18,\n",
       "  17889,\n",
       "  3585,\n",
       "  25,\n",
       "  22691,\n",
       "  420,\n",
       "  374,\n",
       "  2555,\n",
       "  922,\n",
       "  856,\n",
       "  2162,\n",
       "  3224,\n",
       "  11,\n",
       "  384,\n",
       "  13849,\n",
       "  220,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  40,\n",
       "  2846,\n",
       "  6380,\n",
       "  311,\n",
       "  1520,\n",
       "  0,\n",
       "  4452,\n",
       "  11,\n",
       "  358,\n",
       "  1541,\n",
       "  956,\n",
       "  1518,\n",
       "  459,\n",
       "  4652,\n",
       "  12673,\n",
       "  477,\n",
       "  3984,\n",
       "  13,\n",
       "  16910,\n",
       "  499,\n",
       "  4587,\n",
       "  4430,\n",
       "  279,\n",
       "  4652,\n",
       "  449,\n",
       "  757,\n",
       "  779,\n",
       "  358,\n",
       "  649,\n",
       "  63179,\n",
       "  433,\n",
       "  304,\n",
       "  220,\n",
       "  18,\n",
       "  17889,\n",
       "  3585,\n",
       "  369,\n",
       "  499,\n",
       "  30],\n",
       " 'total_duration': 11081718000,\n",
       " 'load_duration': 8152036600,\n",
       " 'prompt_eval_count': 33,\n",
       " 'prompt_eval_duration': 424000000,\n",
       " 'eval_count': 40,\n",
       " 'eval_duration': 2500000000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help! However, I don't see an article attached or provided. Could you please share the article with me so I can summarize it in 3 bullet points for you?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I'm happy to help! However, I don't see an article attached or provided. \"\n",
      " 'Could you please share the article with me so I can summarize it in 3 bullet '\n",
      " 'points for you?')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "pprint(response.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/chat\",\n",
    "    json={\n",
    "        \"model\": \"llama3:8b\",\n",
    "        \"messages\": [ \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello! what are the beautiful things in egypt?\"\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.3,\n",
    "            \"seed\": 101, \n",
    "            \"num_predict\":100, # stop generating at 100 tokens only,\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:8b',\n",
       " 'created_at': '2024-11-25T18:36:09.1719012Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'Egypt is a treasure trove of beauty, rich in history, culture, and natural wonders. Here are some of the most stunning and awe-inspiring attractions:\\n\\n1. **Pyramids of Giza**: The last remaining ancient wonder of the Seven Wonders of the World, these pyramids are an absolute must-see. The Great Pyramid of Khufu is the largest and most impressive.\\n2. **Nile River**: The lifeblood of Egypt, the Nile stretches over 6,695'},\n",
       " 'done_reason': 'length',\n",
       " 'done': True,\n",
       " 'total_duration': 9261657600,\n",
       " 'load_duration': 2987260200,\n",
       " 'prompt_eval_count': 21,\n",
       " 'prompt_eval_duration': 429000000,\n",
       " 'eval_count': 100,\n",
       " 'eval_duration': 5842000000}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Egypt is a treasure trove of beauty, rich in history, culture, and natural '\n",
      " 'wonders. Here are some of the most stunning and awe-inspiring attractions:\\n'\n",
      " '\\n'\n",
      " '1. **Pyramids of Giza**: The last remaining ancient wonder of the Seven '\n",
      " 'Wonders of the World, these pyramids are an absolute must-see. The Great '\n",
      " 'Pyramid of Khufu is the largest and most impressive.\\n'\n",
      " '2. **Nile River**: The lifeblood of Egypt, the Nile stretches over 6,695')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "\n",
    "pprint(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`### num_ctx option -> context window size\n",
    "\n",
    "https://github.com/ollama/ollama/issues/3644#issuecomment-2057646417`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/chat\",\n",
    "    json={\n",
    "        \"model\": \"llama3:8b\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are a detailed story writer for anime\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"\"\"Create a professional story character for anime\"\"\"}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.2,   \n",
    "            \"seed\": 101,\n",
    "            \"num_ctx\" : 8000\n",
    "    }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:8b',\n",
       " 'created_at': '2024-11-25T18:58:02.9606481Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'Here is a professional story character for an anime:\\n\\n**Name:** Kaito Yamato ()\\n\\n**Age:** 17 (High School Student)\\n\\n**Appearance:** Kaito has short, spiky black hair and piercing blue eyes. He stands at around 5\\'9\" with a lean yet athletic build. He often wears his school uniform, but outside of school, he favors casual clothing like hoodies and jeans.\\n\\n**Personality:** Kaito is a free spirit who values independence and self-reliance. As the eldest child in a family of five siblings, he\\'s learned to be resourceful and adaptable. He\\'s fiercely loyal to those he cares about and will go to great lengths to protect them. Despite his tough exterior, Kaito has a soft spot for those in need and is always willing to lend a helping hand.\\n\\n**Background:** Born into a loving but chaotic household, Kaito grew up surrounded by siblings who often drove him crazy. His parents, though well-meaning, were often absent-minded and left the kids to fend for themselves. As a result, Kaito developed strong survival instincts and learned to rely on himself. He\\'s always been fascinated with technology and mechanics, spending countless hours tinkering with gadgets and machinery.\\n\\n**Abilities:**\\n\\n1. **Technopathy:** Kaito has an uncanny ability to understand and repair complex electronic devices. He can hack into computer systems, decode encrypted files, and even create his own software.\\n2. **Mechanical Genius:** With a knack for taking apart and reassembling machines, Kaito is a whiz with tools and machinery. He\\'s always fixing broken appliances or modifying his friends\\' gadgets to make them more efficient.\\n3. **Enhanced Senses:** As a result of his intense focus on technology, Kaito has developed heightened senses that allow him to perceive the world in unique ways. He can see subtle vibrations in electronic signals, hear faint whispers from machinery, and even feel the pulse of energy emanating from devices.\\n\\n**Goals:**\\n\\n1. **Uncover the Truth:** Kaito is determined to uncover the truth behind a series of mysterious events plaguing his town. As he delves deeper into the mystery, he\\'ll encounter powerful forces that threaten the very fabric of reality.\\n2. **Protect His Friends:** With his friends\\' lives on the line, Kaito will stop at nothing to keep them safe from harm. He\\'ll use his skills and abilities to outwit foes, repair damaged equipment, and even create new gadgets to aid in their quest.\\n\\n**Motivations:**\\n\\n1. **Justice:** Kaito believes that justice is key to a harmonious society. As he uncovers the truth behind the mysterious events, he\\'ll strive to bring those responsible to account.\\n2. **Family:** Despite his tough exterior, Kaito\\'s family is everything to him. He\\'ll do whatever it takes to keep them safe and happy.\\n\\n**Conflict:**\\n\\n1. **Inner Turmoil:** As Kaito delves deeper into the mystery, he\\'ll struggle with the weight of responsibility and the fear of losing those he cares about.\\n2. **External Threats:** Powerful forces will stop at nothing to silence Kaito and his friends. He\\'ll face off against ruthless villains, corrupt authorities, and even rival tech-savvy individuals who seek to outdo him.\\n\\n**Themes:**\\n\\n1. **Self-Discovery:** As Kaito navigates the complexities of his world, he\\'ll learn to balance his independence with a sense of responsibility and teamwork.\\n2. **Technology vs. Humanity:** The anime will explore the consequences of relying too heavily on technology, highlighting the importance of human connection and empathy in a rapidly changing world.\\n\\nThis character profile provides a solid foundation for an engaging anime story. Kaito\\'s unique abilities, strong sense of justice, and personal motivations create a compelling narrative that explores themes relevant to modern audiences.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 58540859500,\n",
       " 'load_duration': 20232100,\n",
       " 'prompt_eval_count': 30,\n",
       " 'prompt_eval_duration': 518000000,\n",
       " 'eval_count': 804,\n",
       " 'eval_duration': 58000000000}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What is\\n\\nI'make\"\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "\n",
    "pprint(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llava', 'created_at': '2024-11-25T19:02:56.3292543Z', 'message': {'role': 'assistant', 'content': \" The image shows a bouquet of flowers with green leaves, set against a grassy background. There are three types of flowers: two purple ones that resemble lavender or dill, one yellow flower that could be a daisy or marigold, and some small pink flowers that might be wildflowers or baby's breath. The arrangement is placed on a patch of green grass, and there appears to be a plant with broad leaves in the background. The overall impression is that of a natural, garden-like setting. \"}, 'done_reason': 'stop', 'done': True, 'total_duration': 26571851400, 'load_duration': 12942153100, 'prompt_eval_count': 592, 'prompt_eval_duration': 1509000000, 'eval_count': 111, 'eval_duration': 12117000000}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "   with open(image_path, 'rb') as image_file:\n",
    "       return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def send_image_to_llava(image_path):\n",
    "   base64_image = encode_image_to_base64(image_path)\n",
    "   \n",
    "   response = requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json={\n",
    "            \"model\": \"llava\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"what is in this image?\",\n",
    "                    \"images\": [base64_image]\n",
    "                }\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.3\n",
    "            }\n",
    "        }\n",
    "   )\n",
    "   \n",
    "   return response.json()\n",
    "\n",
    "result = send_image_to_llava(\"./download.jpeg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The image shows a bouquet of flowers with green leaves, set against a '\n",
      " 'grassy background. There are three types of flowers: two purple ones that '\n",
      " 'resemble lavender or dill, one yellow flower that could be a daisy or '\n",
      " \"marigold, and some small pink flowers that might be wildflowers or baby's \"\n",
      " 'breath. The arrangement is placed on a patch of green grass, and there '\n",
      " 'appears to be a plant with broad leaves in the background. The overall '\n",
      " 'impression is that of a natural, garden-like setting. ')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "\n",
    "pprint(result['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENAI\n",
    "Refrences : https://github.com/ollama/ollama/blob/main/docs/openai.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama_fs',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'tell me a joke!',\n",
    "        }\n",
    "    ],\n",
    "    model='llama3:8b',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-557', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that brought a smile to your face!\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732561540, model='llama3:8b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=35, prompt_tokens=15, total_tokens=50, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(chat_completion))\n",
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Chat Response: The Three Laws of Robotics were formulated by science fiction author Isaac Asimov as part of his robot series. The laws are designed to ensure that robots behave in a way that is safe and beneficial to humans. Here are the three laws:\n",
      "\n",
      "1. **A robot may not injure a human being or, through inaction, allow a human being to come to harm**. This law prioritizes the safety of human life above all else.\n",
      "2. **A robot must obey the orders given it by human beings except where such orders would conflict with the First Law**. This law establishes the importance of following human instructions and gives precedence to the well-being of humans over the operations of the robot itself.\n",
      "3. **A robot must protect its own existence as long as such protection does not conflict with the First or Second Law**. This law acknowledges that a robot, like any other self-aware being, has its own interests and requires some protection, but only to the extent that it doesn't compromise human safety or orders.\n",
      "\n",
      "These laws were designed to provide guidelines for robotics development and operation, and they have since become an influential concept in science fiction and science.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama_fs',\n",
    ")\n",
    "\n",
    "def basic_chat():\n",
    "    \"\"\"Basic chat completion example\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant experienced in Robotics science.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What are the three laws of robotics?\"}\n",
    "        ]\n",
    "    )\n",
    "    print(\"Basic Chat Response:\", response.choices[0].message.content)\n",
    "\n",
    "basic_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "refrences : https://github.com/ollama/ollama/blob/main/docs/tutorials/langchainpy.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.13\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. He discovered that shorter wavelengths of light (like blue and violet) are scattered more than longer wavelengths (like red and orange) by tiny molecules of gases in the atmosphere.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. **Sunlight enters Earth's atmosphere**: When sunlight from the sun enters our atmosphere, it contains all the colors of the visible spectrum, including red, orange, yellow, green, blue, indigo, and violet.\n",
      "2. **Molecules scatter light**: The tiny molecules of gases like nitrogen (N2) and oxygen (O2) in the atmosphere are much smaller than the wavelength of light. These molecules randomly collide with the light particles (photons), causing them to change direction.\n",
      "3. **Blue light is scattered more**: Shorter wavelengths, like blue and violet, have a higher frequency and are scattered more efficiently by these tiny molecules. This is because they interact more strongly with the molecules due to their smaller size.\n",
      "4. **Red light continues straight**: Longer wavelengths, like red and orange, have a lower frequency and travel in a relatively straight line, experiencing less scattering.\n",
      "5. **Our eyes perceive the result**: As we look at the sky, our eyes see the combined effect of all this scattered light. The blue and violet light that's been scattered in all directions reaches our eyes from many different angles, making the sky appear blue.\n",
      "\n",
      "In addition to Rayleigh scattering, there are other factors that can affect the color of the sky:\n",
      "\n",
      "* **Dust and water vapor**: Tiny particles in the atmosphere can absorb or scatter certain wavelengths of light, changing the apparent color of the sky.\n",
      "* **Clouds**: Clouds can reflect and scatter sunlight, making the sky appear white or gray.\n",
      "* **Atmospheric conditions**: Atmospheric conditions like pollution, humidity, and temperature can also influence the color of the sky.\n",
      "\n",
      "So, to summarize, the sky appears blue because of the scattering of shorter wavelengths (blue and violet) by tiny molecules in the atmosphere, combined with other factors that can affect the color.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3\"\n",
    ")\n",
    "print(ollama.invoke(\"why is the sky blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egypt is an ancient and storied land, where the majestic Pyramids of Giza stand alongside vibrant souks, mighty Nile River, and a rich cultural heritage that spans thousands of years.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "prompt=\"\"\"\n",
    "write a one line description of egypt\n",
    "\"\"\".strip()\n",
    "\n",
    "print(ollama(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Chain Response: The Northern Lights, also known as the Aurora Borealis, are a breathtaking natural phenomenon that has captivated humans for centuries. The Northern Lights are caused by a complex interaction between charged particles from the sun and the Earth's magnetic field.\n",
      "\n",
      "Here's a detailed explanation of the process:\n",
      "\n",
      "**The Sun's Magnetic Field**\n",
      "\n",
      "The solar wind is a stream of charged particles, mostly electrons and protons, emanating from the sun. These particles are accelerated to high speeds due to the sun's intense magnetic field. When the solar wind reaches the Earth's magnetic field, it interacts with the planet's magnetic poles.\n",
      "\n",
      "**Charged Particles and the Ionosphere**\n",
      "\n",
      "The solar wind is composed of charged particles, which include electrons, protons, and alpha particles (helium nuclei). These particles are attracted to the Earth's magnetic field and are funneled towards the magnetic poles. At the poles, the particles interact with the ionosphere, a region of the atmosphere where the air is ionized by solar radiation.\n",
      "\n",
      "**Excitation of Atmospheric Gases**\n",
      "\n",
      "When the solar wind particles collide with atmospheric gases such as oxygen and nitrogen in the ionosphere, they excite these atoms and molecules. This process causes them to gain energy and move to higher energy states. As a result, the atmosphere is heated, and the excited gas emits light at specific wavelengths.\n",
      "\n",
      "**Emission of Light**\n",
      "\n",
      "There are two types of light emission that occur during the Northern Lights:\n",
      "\n",
      "1. **Excitation-emission**: When the solar wind particles collide with atmospheric gases, they excite these atoms and molecules, causing them to emit light as they return to their ground state.\n",
      "2. **Bremsstrahlung**: When high-energy electrons collide with atomic nuclei in the atmosphere, they release energy in the form of X-rays. These X-rays are then converted into visible light by collisions with atmospheric gases.\n",
      "\n",
      "**Color of the Northern Lights**\n",
      "\n",
      "The color of the Northern Lights depends on the altitude at which the excited gas emits its light:\n",
      "\n",
      "* **Green**: Emitted at altitudes around 100-200 km, this is the most common color and results from the excitation of oxygen atoms.\n",
      "* **Red**: Emitted at higher altitudes (around 200-300 km), this color is produced by the excitation of oxygen molecules.\n",
      "* **Blue**: Less intense than green, this color is also emitted by excited oxygen atoms, but at slightly lower altitudes.\n",
      "\n",
      "**Other Factors that Influence the Northern Lights**\n",
      "\n",
      "While the solar wind and magnetic field are the primary causes of the Northern Lights, other factors can influence their appearance:\n",
      "\n",
      "* **Geomagnetic storms**: Changes in the Earth's magnetic field can cause more intense aurora displays.\n",
      "* **Solar activity**: The frequency and intensity of solar flares and coronal mass ejections (CMEs) can impact the strength of the Northern Lights.\n",
      "* **Atmospheric conditions**: Cloud cover, moon phase, and atmospheric temperature can affect the visibility and brightness of the Northern Lights.\n",
      "\n",
      "In summary, the Northern Lights are a spectacular display of charged particles from the sun interacting with the Earth's magnetic field and atmosphere. The resulting light emission is caused by the excitation and emission of atmospheric gases, producing a range of colors that can be influenced by various factors.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "# 1. Basic Template\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Please provide a detailed answer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=ollama, prompt=prompt)\n",
    "response = chain.invoke({\"question\": \"What causes northern lights?\"})\n",
    "\n",
    "# uncomment for prints\n",
    "# print(type(response))\n",
    "# print(response.keys())\n",
    "# print(response)\n",
    "print(\"Basic Chain Response:\", response['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is Paris the capital of France?\n",
      "Answer: True\n",
      "\n",
      "Question: Is the Earth flat?\n",
      "Answer: False\n",
      "\n",
      "Question: Is Python a programming language?\n",
      "Answer: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import BooleanOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "# Initialize the parser and LLM\n",
    "boolean_parser = BooleanOutputParser()\n",
    "\n",
    "ollama = Ollama(base_url='http://localhost:11434',\n",
    "                model=\"llama3.2\")\n",
    "\n",
    "# Create a template that will yield a yes/no answer\n",
    "template = \"\"\"Given the following question, answer with YES or NO only.\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template)\n",
    "\n",
    "# Create the chain\n",
    "# LCEL -> langchain expression language\n",
    "# https://python.langchain.com/v0.1/docs/expression_language/\n",
    "# Chain = prompt | LLM | Parser\n",
    "\n",
    "chain = prompt | ollama | boolean_parser\n",
    "\n",
    "# Example questions to test\n",
    "questions = [\n",
    "    \"Is Paris the capital of France?\",\n",
    "    \"Is the Earth flat?\",\n",
    "    \"Is Python a programming language?\"\n",
    "]\n",
    "\n",
    "# Run the chain for each question\n",
    "for question in questions:\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    # parsed_result = boolean_parser.parse(result['text'])\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result}\\n\")\n",
    "    # print(f\"parsed Answer: {parsed_result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: pyramid\n",
      "Explanation: ['Ancient Egyptian Construction Methods', 'Pyramid Architecture and Symbolism', \"Theories of Pyramids' Purpose and Functionality\"]\n",
      "\n",
      "Topic: egypt\n",
      "Explanation: ['Egyptian pyramids', 'mummification and burial practices', 'ancient Egyptian hieroglyphic writing system.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "commaseprated_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# generates topics\n",
    "topic_template = \"\"\"Generate three interesting subtopics about: {subject}\n",
    "Output only the subtopics, comma separated.\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "        input_variables=[\"subject\"],\n",
    "        template=topic_template\n",
    "    )\n",
    "\n",
    "topic_chain = prompt | ollama | commaseprated_parser\n",
    "\n",
    "# Run sequential chain\n",
    "subjects = {'egypt', 'pyramid'}\n",
    "\n",
    "for subject in subjects:\n",
    "    # Fix: Changed \"subjects\" to \"subject\" to match the template\n",
    "    Topics = topic_chain.invoke({\"subject\": subject})  \n",
    "    print(f\"\\nTopic: {subject}\")\n",
    "    print(f\"Topics: {Topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review Results:\n",
      "Title: The Matrix\n",
      "Rating: 9\n",
      "Reasons:\n",
      "- Innovative storyline\n",
      "- Impressive special effects\n",
      "- Thought-provoking themes\n",
      "- Strong performances\n",
      "- Visually stunning\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "class MovieReview(BaseModel):\n",
    "    title: str = Field(description=\"The title of the movie\")\n",
    "    rating: int = Field(description=\"Rating from 1-10\")\n",
    "    reasons: List[str] = Field(description=\"Reasons for the rating\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MovieReview)\n",
    "\n",
    "template = \"\"\"Create a movie review in JSON format for the movie: {movie}\n",
    "    \n",
    "    Your response must be a valid JSON object with these fields:\n",
    "    - title (string)\n",
    "    - rating (integer 1-10)\n",
    "    - reasons (array of strings)\n",
    "    \n",
    "    Respond with ONLY the JSON object.\"\"\"\n",
    "\n",
    "# Create prompt\n",
    "prompt = PromptTemplate(\n",
    "        input_variables=[\"movie\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "# full chain with parser\n",
    "chain = prompt | ollama | parser\n",
    "\n",
    "result = chain.invoke({\"movie\": \"The Matrix\"})\n",
    "print(\"\\nReview Results:\")\n",
    "print(f\"Title: {result.title}\")\n",
    "print(f\"Rating: {result.rating}\")\n",
    "print(\"Reasons:\")\n",
    "for reason in result.reasons:\n",
    "    print(f\"- {reason}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugging a memory system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Chat History: \n",
      "Human: What is machine learning?\n",
      "Assistant: Let me help you with that.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Chat History: Human: What is machine learning?\n",
      "AI: Machine learning is a subset of artificial intelligence (AI) that enables systems to automatically improve their performance on a task without being explicitly programmed for that specific task.\n",
      "\n",
      "In traditional programming, a computer program is written with a set of rules and instructions that it follows exactly. In contrast, machine learning algorithms allow the system to learn patterns and relationships in data through experience, and then make predictions or take actions based on those learned patterns.\n",
      "\n",
      "There are several key characteristics of machine learning:\n",
      "\n",
      "1. **Supervised learning**: The system learns from labeled data, where each example is associated with a correct output.\n",
      "2. **Unsupervised learning**: The system discovers patterns and relationships in unlabeled data.\n",
      "3. **Reinforcement learning**: The system learns through trial and error by interacting with an environment.\n",
      "\n",
      "Some common applications of machine learning include:\n",
      "\n",
      "1. Image recognition\n",
      "2. Natural language processing (NLP)\n",
      "3. Predictive analytics\n",
      "4. Speech recognition\n",
      "5. Self-driving cars\n",
      "\n",
      "Machine learning has many benefits, including:\n",
      "\n",
      "1. **Improved accuracy**: By automating the process of learning from data, machine learning algorithms can often achieve higher accuracy than traditional programming methods.\n",
      "2. **Scalability**: Machine learning models can be trained on large datasets, making them well-suited for big data applications.\n",
      "3. **Flexibility**: Machine learning algorithms can be adapted to a wide range of tasks and domains.\n",
      "\n",
      "However, machine learning also has some challenges, including:\n",
      "\n",
      "1. **Data quality**: Machine learning algorithms are only as good as the data they're trained on, so poor-quality or biased data can lead to inaccurate results.\n",
      "2. **Explainability**: It can be difficult to understand why a machine learning model is making certain predictions or decisions.\n",
      "3. **Bias**: Machine learning models can inherit biases from the data they're trained on, leading to unfair outcomes.\n",
      "\n",
      "I hope this helps! Do you have any specific questions about machine learning?\n",
      "Human: Can you give me some examples of its applications?\n",
      "Assistant: Let me help you with that.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Chat History: Human: What is machine learning?\n",
      "AI: Machine learning is a subset of artificial intelligence (AI) that enables systems to automatically improve their performance on a task without being explicitly programmed for that specific task.\n",
      "\n",
      "In traditional programming, a computer program is written with a set of rules and instructions that it follows exactly. In contrast, machine learning algorithms allow the system to learn patterns and relationships in data through experience, and then make predictions or take actions based on those learned patterns.\n",
      "\n",
      "There are several key characteristics of machine learning:\n",
      "\n",
      "1. **Supervised learning**: The system learns from labeled data, where each example is associated with a correct output.\n",
      "2. **Unsupervised learning**: The system discovers patterns and relationships in unlabeled data.\n",
      "3. **Reinforcement learning**: The system learns through trial and error by interacting with an environment.\n",
      "\n",
      "Some common applications of machine learning include:\n",
      "\n",
      "1. Image recognition\n",
      "2. Natural language processing (NLP)\n",
      "3. Predictive analytics\n",
      "4. Speech recognition\n",
      "5. Self-driving cars\n",
      "\n",
      "Machine learning has many benefits, including:\n",
      "\n",
      "1. **Improved accuracy**: By automating the process of learning from data, machine learning algorithms can often achieve higher accuracy than traditional programming methods.\n",
      "2. **Scalability**: Machine learning models can be trained on large datasets, making them well-suited for big data applications.\n",
      "3. **Flexibility**: Machine learning algorithms can be adapted to a wide range of tasks and domains.\n",
      "\n",
      "However, machine learning also has some challenges, including:\n",
      "\n",
      "1. **Data quality**: Machine learning algorithms are only as good as the data they're trained on, so poor-quality or biased data can lead to inaccurate results.\n",
      "2. **Explainability**: It can be difficult to understand why a machine learning model is making certain predictions or decisions.\n",
      "3. **Bias**: Machine learning models can inherit biases from the data they're trained on, leading to unfair outcomes.\n",
      "\n",
      "I hope this helps! Do you have any specific questions about machine learning?\n",
      "Human: Can you give me some examples of its applications?\n",
      "AI: Machine learning has numerous applications across various industries. Here are some examples:\n",
      "\n",
      "1. **Image Recognition**: Facebook uses machine learning to recognize faces in photos and tag them automatically. Google Photos also uses this technology to identify objects, scenes, and people in images.\n",
      "\n",
      "2. **Natural Language Processing (NLP)**: Virtual assistants like Siri, Alexa, and Google Assistant use NLP to understand voice commands and respond accordingly. Chatbots also rely on NLP to communicate with users.\n",
      "\n",
      "3. **Predictive Analytics**: Banks and financial institutions use machine learning to predict customer creditworthiness, detect fraudulent transactions, and forecast stock prices.\n",
      "\n",
      "4. **Speech Recognition**: Speech-to-text systems like Apple's Dictation and Microsoft's Speech Recognition enable users to dictate notes, emails, or messages.\n",
      "\n",
      "5. **Self-Driving Cars**: Companies like Waymo (formerly Google Self-Driving Car project) use machine learning to analyze sensor data from cameras, radar, and lidar, enabling self-driving cars to navigate roads safely.\n",
      "\n",
      "6. **Healthcare**: Machine learning is used in medical diagnosis, drug discovery, and personalized medicine. For instance, IBM's Watson Health uses NLP to analyze medical texts, identify diseases, and provide treatment recommendations.\n",
      "\n",
      "7. **Customer Service**: Chatbots and virtual assistants use machine learning to understand user queries, respond accordingly, and resolve issues efficiently.\n",
      "\n",
      "8. **Recommendation Systems**: Online retailers like Amazon and Netflix use machine learning to recommend products based on user behavior, preferences, and purchase history.\n",
      "\n",
      "9. **Cybersecurity**: Machine learning algorithms can detect and prevent cyber threats by analyzing network traffic patterns, identifying anomalies, and flagging suspicious activity.\n",
      "\n",
      "10. **Autonomous Robots**: Companies like Boston Dynamics and iRobot use machine learning to enable robots to navigate complex environments, avoid obstacles, and adapt to new situations.\n",
      "\n",
      "These examples illustrate the versatility of machine learning in various domains. Do you have any specific area of interest or application you'd like me to expand on?\n",
      "Human: What skills do I need to learn it?\n",
      "Assistant: Let me help you with that.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "# Chat with Memory Example\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\" # is like naming our memory folder\n",
    ")\n",
    "\n",
    "chat_template = \"\"\"\n",
    "Chat History: {chat_history}\n",
    "Human: {human_input}\n",
    "Assistant: Let me help you with that.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    "    template=chat_template\n",
    ")\n",
    "\n",
    "chat_chain = LLMChain(\n",
    "    prompt=chat_prompt,\n",
    "    llm=ollama,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Simulate a conversation\n",
    "responses = [\n",
    "    chat_chain.invoke({\"human_input\": \"What is machine learning?\"}),\n",
    "    chat_chain.invoke({\"human_input\": \"Can you give me some examples of its applications?\"}),\n",
    "    chat_chain.invoke({\"human_input\": \"What skills do I need to learn it?\"})\n",
    "]\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT PROMPTING ===\n",
      "Zero-shot SUCCESS:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Task: Classify this email as spam or legitimate\n",
      "Input: Get rich quick! Click here now!\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: I would classify this email as spam. The language used is sensational and attention-grabbing, with the promise of getting \"rich quick\", which is a common tactic used in phishing and scam emails. The use of the phrase \"Click here now!\" also creates a sense of urgency, which is another characteristic often used in spam emails to trick users into clicking on links or downloading attachments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "print(\"=== ZERO-SHOT PROMPTING ===\")\n",
    "\n",
    "zero_shot_template = \"\"\"\n",
    "Task: {task}\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\", \"input_text\"],\n",
    "    template=zero_shot_template\n",
    ")\n",
    "\n",
    "zero_shot_chain = LLMChain(\n",
    "    prompt=zero_shot_prompt,\n",
    "    llm=ollama,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Success case\n",
    "print(\"Zero-shot SUCCESS:\")\n",
    "success_response = zero_shot_chain.invoke({\n",
    "    \"task\": \"Classify this email as spam or legitimate\",\n",
    "    \"input_text\": \"Get rich quick! Click here now!\"\n",
    "})\n",
    "print(f\"Response: {success_response['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-shot COMPLEX FAILURE (Likely):\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Task: From the text, extract the person's name, their job, and their two main tasks. Format this as a nested JSON object. The top-level key should be 'employee'. This object should contain 'name', 'position', and a 'duties' key which is a list of strings.\n",
      "Input: Dr. Eleanor Vance, the lead botanist, is responsible for cataloging new plant species and monitoring the greenhouse environment.\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: I don't see any text provided. Please provide the text about Dr. Eleanor Vance's job and tasks, and I'll be happy to help you extract this information in the format of a nested JSON object.\n"
     ]
    }
   ],
   "source": [
    "# A more complex failure case\n",
    "print(\"\\nZero-shot COMPLEX FAILURE (Likely):\")\n",
    "complex_failure_response = zero_shot_chain.invoke({\n",
    "    \"task\": \"From the text, extract the person's name, their job, and their two main tasks. Format this as a nested JSON object. The top-level key should be 'employee'. This object should contain 'name', 'position', and a 'duties' key which is a list of strings.\",\n",
    "    \"input_text\": \"Dr. Eleanor Vance, the lead botanist, is responsible for cataloging new plant species and monitoring the greenhouse environment.\"\n",
    "})\n",
    "print(f\"Response: {complex_failure_response['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONE-SHOT PROMPTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEW-SHOT PROMPTING (SOLUTION) ===\n",
      "Few-shot SUCCESS:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Follow the example to extract the required information into the specified JSON format.\n",
      "\n",
      "---\n",
      "Example Output:\n",
      "{\n",
      "    \"employee\": {\n",
      "        \"name\": \"Mr. David Chen\",\n",
      "        \"position\": \"senior software architect\",\n",
      "        \"duties\": [\n",
      "            \"designs the system infrastructure\",\n",
      "            \"leads the code review process\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "---\n",
      "\n",
      "Now, complete the task for the following input.\n",
      "\n",
      "Input: Dr. Eleanor Vance, the lead botanist, is responsible for cataloging new plant species and monitoring the greenhouse environment.\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: Here's how you can extract that information into a JSON format:\n",
      "\n",
      "{\n",
      "    \"employee\": {\n",
      "        \"name\": \"Dr. Eleanor Vance\",\n",
      "        \"position\": \"lead botanist\",\n",
      "        \"duties\": [\n",
      "            \"catalogs new plant species\",\n",
      "            \"monitors the greenhouse environment\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== FEW-SHOT PROMPTING (SOLUTION) ===\")\n",
    "\n",
    "# 1. Define a template with a clear example\n",
    "# The example shows the model exactly how to handle a similar sentence.\n",
    "few_shot_template = \"\"\"\n",
    "Follow the example to extract the required information into the specified JSON format.\n",
    "\n",
    "---\n",
    "Example Output:\n",
    "{{\n",
    "    \"employee\": {{\n",
    "        \"name\": \"Mr. David Chen\",\n",
    "        \"position\": \"senior software architect\",\n",
    "        \"duties\": [\n",
    "            \"designs the system infrastructure\",\n",
    "            \"leads the code review process\"\n",
    "        ]\n",
    "    }}\n",
    "}}\n",
    "---\n",
    "\n",
    "Now, complete the task for the following input.\n",
    "\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create a new PromptTemplate and Chain for the few-shot task\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=few_shot_template,\n",
    "    input_variables=[\"input_text\"]\n",
    ")\n",
    "\n",
    "few_shot_chain = LLMChain(\n",
    "    prompt=few_shot_prompt,\n",
    "    llm=ollama,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3. Invoke the chain with the same input that previously failed\n",
    "print(\"Few-shot SUCCESS:\")\n",
    "success_response = few_shot_chain.invoke({\n",
    "    \"input_text\": \"Dr. Eleanor Vance, the lead botanist, is responsible for cataloging new plant species and monitoring the greenhouse environment.\"\n",
    "})\n",
    "\n",
    "# The response should now be the correctly formatted JSON\n",
    "print(f\"Response: {success_response['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few Shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MULTI-SHOT (FEW-SHOT) EXAMPLE: TICKET CLASSIFICATION ===\n",
      "Classifying a new, more complex support ticket:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a support ticket classifier. Analyze the user's message and classify it into one of the following categories: 'Billing Issue', 'Technical Support', or 'General Inquiry'.\n",
      "Assign a priority of 'High', 'Medium', or 'Low' based on the message's urgency. Provide the output in a JSON format.\n",
      "\n",
      "---\n",
      "Example 1\n",
      "Input: \"Hi, I can't log in to my dashboard. The password reset link you sent me is broken and I have an urgent deadline. Please help!\"\n",
      "Output:\n",
      "{\n",
      "    \"category\": \"Technical Support\",\n",
      "    \"priority\": \"High\"\n",
      "}\n",
      "---\n",
      "Example 2\n",
      "Input: \"I was looking at my last invoice and I believe I was overcharged. My plan is $25/month but I was billed for $50. Can you please correct this?\"\n",
      "Output:\n",
      "{\n",
      "    \"category\": \"Billing Issue\",\n",
      "    \"priority\": \"Medium\"\n",
      "}\n",
      "---\n",
      "Example 3\n",
      "Input: \"Hello, I was just wondering where I can find your company's privacy policy documentation on the website?\"\n",
      "Output:\n",
      "{\n",
      "    \"category\": \"General Inquiry\",\n",
      "    \"priority\": \"Low\"\n",
      "}\n",
      "---\n",
      "\n",
      "Now, complete the task for the following input.\n",
      "\n",
      "Input: My app keeps crashing every time I try to upload a file. It's not a complete outage, but it's really slowing down my work. What should I do?\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: {\n",
      "    \"category\": \"Technical Support\",\n",
      "    \"priority\": \"Medium\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== MULTI-SHOT (FEW-SHOT) EXAMPLE: TICKET CLASSIFICATION ===\")\n",
    "\n",
    "# 1. Define a template with MULTIPLE examples to teach the model the nuances\n",
    "# of the classification logic.\n",
    "# Each example is clearly separated.\n",
    "multi_shot_template = \"\"\"\n",
    "You are a support ticket classifier. Analyze the user's message and classify it into one of the following categories: 'Billing Issue', 'Technical Support', or 'General Inquiry'.\n",
    "Assign a priority of 'High', 'Medium', or 'Low' based on the message's urgency. Provide the output in a JSON format.\n",
    "\n",
    "---\n",
    "Example 1\n",
    "Input: \"Hi, I can't log in to my dashboard. The password reset link you sent me is broken and I have an urgent deadline. Please help!\"\n",
    "Output:\n",
    "{{\n",
    "    \"category\": \"Technical Support\",\n",
    "    \"priority\": \"High\"\n",
    "}}\n",
    "---\n",
    "Example 2\n",
    "Input: \"I was looking at my last invoice and I believe I was overcharged. My plan is $25/month but I was billed for $50. Can you please correct this?\"\n",
    "Output:\n",
    "{{\n",
    "    \"category\": \"Billing Issue\",\n",
    "    \"priority\": \"Medium\"\n",
    "}}\n",
    "---\n",
    "Example 3\n",
    "Input: \"Hello, I was just wondering where I can find your company's privacy policy documentation on the website?\"\n",
    "Output:\n",
    "{{\n",
    "    \"category\": \"General Inquiry\",\n",
    "    \"priority\": \"Low\"\n",
    "}}\n",
    "---\n",
    "\n",
    "Now, complete the task for the following input.\n",
    "\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create the PromptTemplate and Chain\n",
    "multi_shot_prompt = PromptTemplate(\n",
    "    template=multi_shot_template,\n",
    "    input_variables=[\"input_text\"]\n",
    ")\n",
    "\n",
    "multi_shot_chain = LLMChain(\n",
    "    prompt=multi_shot_prompt,\n",
    "    llm=ollama,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3. Invoke the chain with a new, unseen ticket that requires nuanced understanding\n",
    "print(\"Classifying a new, more complex support ticket:\")\n",
    "final_ticket_response = multi_shot_chain.invoke({\n",
    "    \"input_text\": \"My app keeps crashing every time I try to upload a file. It's not a complete outage, but it's really slowing down my work. What should I do?\"\n",
    "})\n",
    "\n",
    "print(f\"Response: {final_ticket_response['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COT thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<Screenshot 2025-06-22 224246.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Planning: Here are the 3-4 main steps to solve this problem:\n",
      "\n",
      "1. Calculate the distance traveled at 60 mph for 2.5 hours\n",
      "2. Calculate the distance traveled at 80 mph for 1.5 hours\n",
      "3. Add the two distances together\n",
      "4. Consider if the times should be converted to just hours (they already are)\n",
      "\n",
      "Step 2 - Reasoning: To solve this problem, let's follow the steps you provided:\n",
      "\n",
      "Step 1: Calculate the distance traveled at 60 mph for 2.5 hours\n",
      "To calculate the distance traveled, we can use the formula:\n",
      "Distance = Speed x Time\n",
      "\n",
      "Given speed: 60 mph\n",
      "Given time: 2.5 hours\n",
      "\n",
      "Distance = 60 x 2.5\n",
      "Distance = 150 miles\n",
      "\n",
      "So, the train travels 150 miles in the first part of its journey.\n",
      "\n",
      "Step 2: Calculate the distance traveled at 80 mph for 1.5 hours\n",
      "Using the same formula:\n",
      "Given speed: 80 mph\n",
      "Given time: 1.5 hours\n",
      "\n",
      "Distance = 80 x 1.5\n",
      "Distance = 120 miles\n",
      "\n",
      "So, the train travels 120 miles in the second part of its journey.\n",
      "\n",
      "Step 3: Add the two distances together\n",
      "Total distance traveled = Distance in first part + Distance in second part\n",
      "Total distance traveled = 150 miles + 120 miles\n",
      "Total distance traveled = 270 miles\n",
      "\n",
      "Therefore, the total distance traveled by the train is 270 miles.\n",
      "\n",
      "Step 3 - Final Answer: Your reasoning is spot on! You followed the correct steps to calculate the distances and then add them together. The final answer is:\n",
      "\n",
      "The total distance traveled by the train is indeed: **270 miles**.\n",
      "\n",
      "Well done! Your understanding of the formula (Distance = Speed x Time) and application of it in this problem is excellent.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2:1b\" \n",
    ")\n",
    "\n",
    "# Step 1: Planning Chain\n",
    "planning_template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "What are the key steps I need to think about to solve this problem?\n",
    "List 3-4 main steps, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "planning_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=planning_template\n",
    ")\n",
    "\n",
    "planning_chain = LLMChain(llm=ollama, prompt=planning_prompt)\n",
    "\n",
    "# Step 2: Reasoning Chain  \n",
    "reasoning_template = \"\"\"\n",
    "Question: {question}\n",
    "Steps to follow: {steps}\n",
    "\n",
    "Now work through each step carefully and show your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"steps\"],\n",
    "    template=reasoning_template\n",
    ")\n",
    "\n",
    "reasoning_chain = LLMChain(llm=ollama, prompt=reasoning_prompt)\n",
    "\n",
    "# Step 3: Final Answer Chain\n",
    "answer_template = \"\"\"\n",
    "Question: {question}\n",
    "My reasoning: {reasoning}\n",
    "\n",
    "Based on my reasoning above, what is the final clear answer?\n",
    "\"\"\"\n",
    "\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"reasoning\"],\n",
    "    template=answer_template\n",
    ")\n",
    "\n",
    "answer_chain = LLMChain(llm=ollama, prompt=answer_prompt)\n",
    "\n",
    "# Run the multi-step chain\n",
    "question = \"If a train travels 60 mph for 2.5 hours, then 80 mph for 1.5 hours, what's the total distance?\"\n",
    "\n",
    "# Call 1: Get the steps\n",
    "steps_response = planning_chain.invoke({\"question\": question})\n",
    "print(\"Step 1 - Planning:\", steps_response['text'])\n",
    "\n",
    "# Call 2: Do the reasoning\n",
    "reasoning_response = reasoning_chain.invoke({\n",
    "    \"question\": question, \n",
    "    \"steps\": steps_response['text']\n",
    "})\n",
    "print(\"\\nStep 2 - Reasoning:\", reasoning_response['text'])\n",
    "\n",
    "# Call 3: Get final answer\n",
    "final_response = answer_chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"reasoning\": reasoning_response['text']\n",
    "})\n",
    "print(\"\\nStep 3 - Final Answer:\", final_response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Area calculation ===\n",
      "LLM wants to call: calculate_area(5, 3)\n",
      "Function result: The area is 15.0 square units\n",
      "Final Answer: To solve this problem, we can use the formula for the area of a rectangle:\n",
      "\n",
      "Area = Length x Width\n",
      "\n",
      "We are given that the length of the rectangle is 5 meters and the width is 3 meters. Plugging these values into the formula, we get:\n",
      "\n",
      "Area = 5 x 3\n",
      "= 15\n",
      "\n",
      "This matches the function result provided. Therefore, the area of the rectangle is indeed 15 square units.\n",
      "\n",
      "=== Test 2: Weather query ===\n",
      "LLM wants to call: get_weather(\"Cairo\")\n",
      "Function result: Hot, 95F\n",
      "Final Answer: I'm happy to help you with your question! However, I don't have real-time access to current weather conditions. But I can suggest some ways for you to find out the current weather in Cairo.\n",
      "\n",
      "You can check online weather websites or apps such as AccuWeather, Weather.com, or BBC Weather, which provide up-to-date weather forecasts and conditions for cities around the world, including Cairo.\n",
      "\n",
      "Alternatively, you can also check social media or news websites that often post updates on current weather conditions.\n",
      "\n",
      "That being said, based on historical climate data, I can tell you that Cairo is known to have a hot desert climate, with very little rainfall throughout the year. During the summer months, temperatures can reach as high as 95F (35C) and above, making it one of the hottest cities in the world.\n",
      "\n",
      "So while I don't have real-time information on the current weather in Cairo, I hope this gives you an idea of what to expect during different times of the year.\n",
      "\n",
      "=== Test 3: Regular question ===\n",
      "LLM wants to call: get_weather(\"Fairbanks\")\n",
      "Function result: Weather data not available\n",
      "Final Answer: The Northern Lights, also known as the Aurora Borealis, are a natural phenomenon that occurs when charged particles from the sun interact with the Earth's magnetic field and atmosphere. The main cause of the Northern Lights is the solar wind, which is a stream of charged particles emitted by the sun during solar flares and coronal mass ejections.\n",
      "\n",
      "Here's a step-by-step explanation:\n",
      "\n",
      "1. **Solar Wind**: The solar wind is a high-energy plasma that consists of electrons, protons, and alpha particles (helium nuclei). During intense solar activity, such as solar flares or coronal mass ejections, the solar wind increases in intensity.\n",
      "2. **Magnetic Field**: When the solar wind reaches the Earth's magnetic field, it is redirected towards the poles by the magnetic field lines. The magnetic field acts as a shield, deflecting the charged particles away from the equator and towards the poles.\n",
      "3. **Atmospheric Interaction**: At the poles, the solar wind particles collide with the Earth's atmosphere, causing the atoms and molecules in the air to become excited. This excitation leads to the emission of light at specific wavelengths, which we see as the Northern Lights.\n",
      "4. **Color and Movement**: The color of the Northern Lights depends on the energy of the particles and the altitude at which they collide with the atmosphere. Green is the most common color, produced by collisions at altitudes of around 100-200 km. Red is produced by collisions at higher altitudes, while blue and violet are produced by collisions at lower altitudes.\n",
      "5. **Aurora Season**: The Northern Lights are typically visible on clear, dark nights from late August to early April, when the Earth's tilt towards the sun results in more solar wind particles interacting with our magnetic field.\n",
      "\n",
      "In summary, the Northern Lights are caused by the interaction between the solar wind and the Earth's magnetic field, leading to the excitation of atmospheric atoms and molecules, which emit light as we see the aurora.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\"\n",
    ")\n",
    "\n",
    "# Simple function to call\n",
    "def calculate_area(length, width):\n",
    "    \"\"\"Calculate area of a rectangle\"\"\"\n",
    "    result = length * width\n",
    "    return f\"The area is {result} square units\"\n",
    "\n",
    "def get_weather(city):\n",
    "    \"\"\"Mock weather function\"\"\"\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 75F\",\n",
    "        \"London\": \"Rainy, 60F\", \n",
    "        \"Cairo\": \"Hot, 95F\"\n",
    "    }\n",
    "    return weather_data.get(city, \"Weather data not available\")\n",
    "\n",
    "# Function calling template\n",
    "function_template = \"\"\"\n",
    "You can call these functions:\n",
    "- calculate_area(length, width) - calculates rectangle area\n",
    "- get_weather(city) - gets weather for a city\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If you need to use a function, respond with:\n",
    "FUNCTION_CALL: function_name(param1, param2)\n",
    "\n",
    "Otherwise, just answer normally.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=function_template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=ollama, prompt=prompt)\n",
    "\n",
    "def process_response(question):\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    answer = response['text']\n",
    "    \n",
    "    # Check if LLM wants to call a function\n",
    "    if \"FUNCTION_CALL:\" in answer:\n",
    "        # Extract function call\n",
    "        match = re.search(r'FUNCTION_CALL:\\s*(\\w+)\\((.*?)\\)', answer)\n",
    "        if match:\n",
    "            func_name = match.group(1)\n",
    "            params = match.group(2)\n",
    "            \n",
    "            print(f\"LLM wants to call: {func_name}({params})\")\n",
    "            \n",
    "            # Execute the function\n",
    "            if func_name == \"calculate_area\":\n",
    "                # Parse parameters\n",
    "                nums = [float(x.strip()) for x in params.split(',')]\n",
    "                result = calculate_area(nums[0], nums[1])\n",
    "                \n",
    "            elif func_name == \"get_weather\":\n",
    "                city = params.strip().strip('\"\\'')\n",
    "                result = get_weather(city)\n",
    "            \n",
    "            print(f\"Function result: {result}\")\n",
    "            \n",
    "            # Get final response with function result\n",
    "            final_template = \"\"\"\n",
    "            Question: {question}\n",
    "            Function result: {function_result}\n",
    "            \n",
    "            Please provide a complete answer using this function result.\n",
    "            \"\"\"\n",
    "            \n",
    "            final_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"function_result\"],\n",
    "                template=final_template\n",
    "            )\n",
    "            \n",
    "            final_chain = LLMChain(llm=ollama, prompt=final_prompt)\n",
    "            final_response = final_chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"function_result\": result\n",
    "            })\n",
    "            \n",
    "            return final_response['text']\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the function calling\n",
    "print(\"=== Test 1: Area calculation ===\")\n",
    "result1 = process_response(\"What's the area of a rectangle that is 5 meters long and 3 meters wide?\")\n",
    "print(\"Final Answer:\", result1)\n",
    "\n",
    "print(\"\\n=== Test 2: Weather query ===\")\n",
    "result2 = process_response(\"What's the weather like in Cairo?\")\n",
    "print(\"Final Answer:\", result2)\n",
    "\n",
    "print(\"\\n=== Test 3: Regular question ===\")\n",
    "result3 = process_response(\"What causes northern lights?\")\n",
    "print(\"Final Answer:\", result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some personal data we want to chat with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= \"\"\"\n",
    "('Egypt is a treasure trove of breathtaking beauty, rich history, and vibrant '\n",
    " 'culture. Here are some of the most stunning aspects of Egypt:\\n'\n",
    " '\\n'\n",
    " '1. Pyramids of Giza: The last remaining ancient wonder of the Seven Wonders '\n",
    " 'of the Ancient World, the Pyramids of Giza are an awe-inspiring sight. The '\n",
    " 'largest pyramid, the Great Pyramid of Khufu, is an engineering marvel that '\n",
    " 'stands tall at over 481 feet (147 meters) high.\\n'\n",
    " '2. Nile River: The longest river in the world, the Nile flows through Egypt '\n",
    " 'and supports a rich ecosystem, including lush valleys, sandy deserts, and '\n",
    " 'majestic temples along its banks.\\n'\n",
    " '3. Valley of the Kings: Located on the west bank of the Nile, this vast '\n",
    " 'burial ground is home to over 60 pharaohs and their families, with elaborate '\n",
    " 'tombs carved into the limestone cliffs.\\n'\n",
    " '4. Temple of Karnak: One of the largest temple complexes in the world, the '\n",
    " 'Temple of Karnak is a stunning example of ancient Egyptian architecture, '\n",
    " 'featuring intricate hieroglyphics, obelisks, and grand halls.\\n'\n",
    " '5. Abu Simbel Temples: Located on the island of Philae in the Nile River, '\n",
    " 'these massive temples were carved out of solid rock to house the tombs of '\n",
    " 'Pharaoh Ramses II and his wife Nefertari.\\n'\n",
    " \"6. Red Sea Beaches: Egypt's coastline offers some of the most beautiful \"\n",
    " 'beaches in the world, with crystal-clear waters, white sand, and coral reefs '\n",
    " 'perfect for snorkeling and scuba diving.\\n'\n",
    " '7. Luxor Temple: This magnificent temple complex is one of the '\n",
    " 'best-preserved ancient structures in Egypt, featuring stunning reliefs, '\n",
    " 'columns, and a grand entrance.\\n'\n",
    " '8. Valley of the Queens: Located near Luxor, this burial ground is home to '\n",
    " 'over 100 tombs, including those of Pharaoh Hatshepsut and her sister '\n",
    " 'Nefertiti.\\n'\n",
    " \"9. Egyptian Museum: One of the world's greatest museums, the Egyptian Museum \"\n",
    " 'in Cairo houses an incredible collection of artifacts from ancient Egypt, '\n",
    " 'including mummies, sarcophagi, and other treasures.\\n'\n",
    " '10. Nile River Cruises: Take a relaxing boat ride along the Nile, exploring '\n",
    " 'the countryside, visiting temples, and enjoying the scenic views of the '\n",
    " 'river.\\n'\n",
    " '\\n'\n",
    " 'These are just a few of the many beautiful things to see in Egypt. With its '\n",
    " 'rich history, stunning landscapes, and vibrant culture, Egypt is a '\n",
    " 'destination that will leave you in awe.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=5)\n",
    "all_splits = text_splitter.split_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Egypt is a treasure trove of breathtaking\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding model\n",
    "\n",
    "ollama pull nomic-embed-text\n",
    "\n",
    "Pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_texts(texts=all_splits, embedding=oembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"what can you tell me about Pyramids of Giza?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"'of the Ancient World, the Pyramids of Giza are\"),\n",
       " Document(page_content=\"'1. Pyramids of Giza: The last remaining ancient\"),\n",
       " Document(page_content=\"'largest pyramid, the Great Pyramid of Khufu, is\"),\n",
       " Document(page_content=\"'best-preserved ancient structures in Egypt,\")]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
