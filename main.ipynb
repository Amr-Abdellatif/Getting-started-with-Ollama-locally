{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Ollama & Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Have two API's to be consider natively by Ollama : \n",
    "#### 1. Generate\n",
    "#### 2. chat\n",
    "Reference: \n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "text = \"Hello this is something about my home country, egypt \"\n",
    "\n",
    "response = requests.post('http://localhost:11434/api/generate', \n",
    "        json={\n",
    "            \"model\": \"llama3:8b\",\n",
    "            \"prompt\": f\"Summarize this article in 3 bullet points: {text}\",\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.3,\n",
    "            }}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:8b',\n",
       " 'created_at': '2024-11-25T17:40:16.0643624Z',\n",
       " 'response': \"I'm happy to help! However, I don't see an article attached or provided. Could you please share the article with me so I can summarize it in 3 bullet points for you?\",\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'context': [128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  9370,\n",
       "  5730,\n",
       "  553,\n",
       "  420,\n",
       "  4652,\n",
       "  304,\n",
       "  220,\n",
       "  18,\n",
       "  17889,\n",
       "  3585,\n",
       "  25,\n",
       "  22691,\n",
       "  420,\n",
       "  374,\n",
       "  2555,\n",
       "  922,\n",
       "  856,\n",
       "  2162,\n",
       "  3224,\n",
       "  11,\n",
       "  384,\n",
       "  13849,\n",
       "  220,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  40,\n",
       "  2846,\n",
       "  6380,\n",
       "  311,\n",
       "  1520,\n",
       "  0,\n",
       "  4452,\n",
       "  11,\n",
       "  358,\n",
       "  1541,\n",
       "  956,\n",
       "  1518,\n",
       "  459,\n",
       "  4652,\n",
       "  12673,\n",
       "  477,\n",
       "  3984,\n",
       "  13,\n",
       "  16910,\n",
       "  499,\n",
       "  4587,\n",
       "  4430,\n",
       "  279,\n",
       "  4652,\n",
       "  449,\n",
       "  757,\n",
       "  779,\n",
       "  358,\n",
       "  649,\n",
       "  63179,\n",
       "  433,\n",
       "  304,\n",
       "  220,\n",
       "  18,\n",
       "  17889,\n",
       "  3585,\n",
       "  369,\n",
       "  499,\n",
       "  30],\n",
       " 'total_duration': 11081718000,\n",
       " 'load_duration': 8152036600,\n",
       " 'prompt_eval_count': 33,\n",
       " 'prompt_eval_duration': 424000000,\n",
       " 'eval_count': 40,\n",
       " 'eval_duration': 2500000000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help! However, I don't see an article attached or provided. Could you please share the article with me so I can summarize it in 3 bullet points for you?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I'm happy to help! However, I don't see an article attached or provided. \"\n",
      " 'Could you please share the article with me so I can summarize it in 3 bullet '\n",
      " 'points for you?')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "pprint(response.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/chat\",\n",
    "    json={\n",
    "        \"model\": \"llama3:8b\",\n",
    "        \"messages\": [ \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello! what are the beautiful things in egypt?\"\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.3,\n",
    "            \"seed\": 101, \n",
    "            \"num_predict\":100, # stop generating at 100 tokens only,\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:8b',\n",
       " 'created_at': '2024-11-25T18:36:09.1719012Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'Egypt is a treasure trove of beauty, rich in history, culture, and natural wonders. Here are some of the most stunning and awe-inspiring attractions:\\n\\n1. **Pyramids of Giza**: The last remaining ancient wonder of the Seven Wonders of the World, these pyramids are an absolute must-see. The Great Pyramid of Khufu is the largest and most impressive.\\n2. **Nile River**: The lifeblood of Egypt, the Nile stretches over 6,695'},\n",
       " 'done_reason': 'length',\n",
       " 'done': True,\n",
       " 'total_duration': 9261657600,\n",
       " 'load_duration': 2987260200,\n",
       " 'prompt_eval_count': 21,\n",
       " 'prompt_eval_duration': 429000000,\n",
       " 'eval_count': 100,\n",
       " 'eval_duration': 5842000000}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Egypt is a treasure trove of beauty, rich in history, culture, and natural '\n",
      " 'wonders. Here are some of the most stunning and awe-inspiring attractions:\\n'\n",
      " '\\n'\n",
      " '1. **Pyramids of Giza**: The last remaining ancient wonder of the Seven '\n",
      " 'Wonders of the World, these pyramids are an absolute must-see. The Great '\n",
      " 'Pyramid of Khufu is the largest and most impressive.\\n'\n",
      " '2. **Nile River**: The lifeblood of Egypt, the Nile stretches over 6,695')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "\n",
    "pprint(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`### num_ctx option -> context window size\n",
    "\n",
    "https://github.com/ollama/ollama/issues/3644#issuecomment-2057646417`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/chat\",\n",
    "    json={\n",
    "        \"model\": \"llama3:8b\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are a detailed story writer for anime\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"\"\"Create a professional story character for anime\"\"\"}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.2,   \n",
    "            \"seed\": 101,\n",
    "            \"num_ctx\" : 8000\n",
    "    }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:8b',\n",
       " 'created_at': '2024-11-25T18:58:02.9606481Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'Here is a professional story character for an anime:\\n\\n**Name:** Kaito Yamato ()\\n\\n**Age:** 17 (High School Student)\\n\\n**Appearance:** Kaito has short, spiky black hair and piercing blue eyes. He stands at around 5\\'9\" with a lean yet athletic build. He often wears his school uniform, but outside of school, he favors casual clothing like hoodies and jeans.\\n\\n**Personality:** Kaito is a free spirit who values independence and self-reliance. As the eldest child in a family of five siblings, he\\'s learned to be resourceful and adaptable. He\\'s fiercely loyal to those he cares about and will go to great lengths to protect them. Despite his tough exterior, Kaito has a soft spot for those in need and is always willing to lend a helping hand.\\n\\n**Background:** Born into a loving but chaotic household, Kaito grew up surrounded by siblings who often drove him crazy. His parents, though well-meaning, were often absent-minded and left the kids to fend for themselves. As a result, Kaito developed strong survival instincts and learned to rely on himself. He\\'s always been fascinated with technology and mechanics, spending countless hours tinkering with gadgets and machinery.\\n\\n**Abilities:**\\n\\n1. **Technopathy:** Kaito has an uncanny ability to understand and repair complex electronic devices. He can hack into computer systems, decode encrypted files, and even create his own software.\\n2. **Mechanical Genius:** With a knack for taking apart and reassembling machines, Kaito is a whiz with tools and machinery. He\\'s always fixing broken appliances or modifying his friends\\' gadgets to make them more efficient.\\n3. **Enhanced Senses:** As a result of his intense focus on technology, Kaito has developed heightened senses that allow him to perceive the world in unique ways. He can see subtle vibrations in electronic signals, hear faint whispers from machinery, and even feel the pulse of energy emanating from devices.\\n\\n**Goals:**\\n\\n1. **Uncover the Truth:** Kaito is determined to uncover the truth behind a series of mysterious events plaguing his town. As he delves deeper into the mystery, he\\'ll encounter powerful forces that threaten the very fabric of reality.\\n2. **Protect His Friends:** With his friends\\' lives on the line, Kaito will stop at nothing to keep them safe from harm. He\\'ll use his skills and abilities to outwit foes, repair damaged equipment, and even create new gadgets to aid in their quest.\\n\\n**Motivations:**\\n\\n1. **Justice:** Kaito believes that justice is key to a harmonious society. As he uncovers the truth behind the mysterious events, he\\'ll strive to bring those responsible to account.\\n2. **Family:** Despite his tough exterior, Kaito\\'s family is everything to him. He\\'ll do whatever it takes to keep them safe and happy.\\n\\n**Conflict:**\\n\\n1. **Inner Turmoil:** As Kaito delves deeper into the mystery, he\\'ll struggle with the weight of responsibility and the fear of losing those he cares about.\\n2. **External Threats:** Powerful forces will stop at nothing to silence Kaito and his friends. He\\'ll face off against ruthless villains, corrupt authorities, and even rival tech-savvy individuals who seek to outdo him.\\n\\n**Themes:**\\n\\n1. **Self-Discovery:** As Kaito navigates the complexities of his world, he\\'ll learn to balance his independence with a sense of responsibility and teamwork.\\n2. **Technology vs. Humanity:** The anime will explore the consequences of relying too heavily on technology, highlighting the importance of human connection and empathy in a rapidly changing world.\\n\\nThis character profile provides a solid foundation for an engaging anime story. Kaito\\'s unique abilities, strong sense of justice, and personal motivations create a compelling narrative that explores themes relevant to modern audiences.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 58540859500,\n",
       " 'load_duration': 20232100,\n",
       " 'prompt_eval_count': 30,\n",
       " 'prompt_eval_duration': 518000000,\n",
       " 'eval_count': 804,\n",
       " 'eval_duration': 58000000000}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What is\\n\\nI'make\"\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "\n",
    "pprint(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llava', 'created_at': '2024-11-25T19:02:56.3292543Z', 'message': {'role': 'assistant', 'content': \" The image shows a bouquet of flowers with green leaves, set against a grassy background. There are three types of flowers: two purple ones that resemble lavender or dill, one yellow flower that could be a daisy or marigold, and some small pink flowers that might be wildflowers or baby's breath. The arrangement is placed on a patch of green grass, and there appears to be a plant with broad leaves in the background. The overall impression is that of a natural, garden-like setting. \"}, 'done_reason': 'stop', 'done': True, 'total_duration': 26571851400, 'load_duration': 12942153100, 'prompt_eval_count': 592, 'prompt_eval_duration': 1509000000, 'eval_count': 111, 'eval_duration': 12117000000}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "   with open(image_path, 'rb') as image_file:\n",
    "       return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def send_image_to_llava(image_path):\n",
    "   base64_image = encode_image_to_base64(image_path)\n",
    "   \n",
    "   response = requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json={\n",
    "            \"model\": \"llava\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"what is in this image?\",\n",
    "                    \"images\": [base64_image]\n",
    "                }\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.3\n",
    "            }\n",
    "        }\n",
    "   )\n",
    "   \n",
    "   return response.json()\n",
    "\n",
    "result = send_image_to_llava(\"./download.jpeg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The image shows a bouquet of flowers with green leaves, set against a '\n",
      " 'grassy background. There are three types of flowers: two purple ones that '\n",
      " 'resemble lavender or dill, one yellow flower that could be a daisy or '\n",
      " \"marigold, and some small pink flowers that might be wildflowers or baby's \"\n",
      " 'breath. The arrangement is placed on a patch of green grass, and there '\n",
      " 'appears to be a plant with broad leaves in the background. The overall '\n",
      " 'impression is that of a natural, garden-like setting. ')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "\n",
    "pprint(result['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENAI\n",
    "Refrences : https://github.com/ollama/ollama/blob/main/docs/openai.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama_fs',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'tell me a joke!',\n",
    "        }\n",
    "    ],\n",
    "    model='llama3:8b',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-557', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that brought a smile to your face!\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732561540, model='llama3:8b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=35, prompt_tokens=15, total_tokens=50, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(chat_completion))\n",
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Chat Response: The Three Laws of Robotics were formulated by science fiction author Isaac Asimov as part of his robot series. The laws are designed to ensure that robots behave in a way that is safe and beneficial to humans. Here are the three laws:\n",
      "\n",
      "1. **A robot may not injure a human being or, through inaction, allow a human being to come to harm**. This law prioritizes the safety of human life above all else.\n",
      "2. **A robot must obey the orders given it by human beings except where such orders would conflict with the First Law**. This law establishes the importance of following human instructions and gives precedence to the well-being of humans over the operations of the robot itself.\n",
      "3. **A robot must protect its own existence as long as such protection does not conflict with the First or Second Law**. This law acknowledges that a robot, like any other self-aware being, has its own interests and requires some protection, but only to the extent that it doesn't compromise human safety or orders.\n",
      "\n",
      "These laws were designed to provide guidelines for robotics development and operation, and they have since become an influential concept in science fiction and science.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama_fs',\n",
    ")\n",
    "\n",
    "def basic_chat():\n",
    "    \"\"\"Basic chat completion example\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant experienced in Robotics science.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What are the three laws of robotics?\"}\n",
    "        ]\n",
    "    )\n",
    "    print(\"Basic Chat Response:\", response.choices[0].message.content)\n",
    "\n",
    "basic_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "refrences : https://github.com/ollama/ollama/blob/main/docs/tutorials/langchainpy.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.13\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. He discovered that shorter wavelengths of light (like blue and violet) are scattered more than longer wavelengths (like red and orange) by tiny molecules of gases in the atmosphere.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. **Sunlight enters Earth's atmosphere**: When sunlight from the sun enters our atmosphere, it contains all the colors of the visible spectrum, including red, orange, yellow, green, blue, indigo, and violet.\n",
      "2. **Molecules scatter light**: The tiny molecules of gases like nitrogen (N2) and oxygen (O2) in the atmosphere are much smaller than the wavelength of light. These molecules randomly collide with the light particles (photons), causing them to change direction.\n",
      "3. **Blue light is scattered more**: Shorter wavelengths, like blue and violet, have a higher frequency and are scattered more efficiently by these tiny molecules. This is because they interact more strongly with the molecules due to their smaller size.\n",
      "4. **Red light continues straight**: Longer wavelengths, like red and orange, have a lower frequency and travel in a relatively straight line, experiencing less scattering.\n",
      "5. **Our eyes perceive the result**: As we look at the sky, our eyes see the combined effect of all this scattered light. The blue and violet light that's been scattered in all directions reaches our eyes from many different angles, making the sky appear blue.\n",
      "\n",
      "In addition to Rayleigh scattering, there are other factors that can affect the color of the sky:\n",
      "\n",
      "* **Dust and water vapor**: Tiny particles in the atmosphere can absorb or scatter certain wavelengths of light, changing the apparent color of the sky.\n",
      "* **Clouds**: Clouds can reflect and scatter sunlight, making the sky appear white or gray.\n",
      "* **Atmospheric conditions**: Atmospheric conditions like pollution, humidity, and temperature can also influence the color of the sky.\n",
      "\n",
      "So, to summarize, the sky appears blue because of the scattering of shorter wavelengths (blue and violet) by tiny molecules in the atmosphere, combined with other factors that can affect the color.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3\"\n",
    ")\n",
    "print(ollama.invoke(\"why is the sky blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egypt is an ancient and storied land, where the majestic Pyramids of Giza stand alongside vibrant souks, mighty Nile River, and a rich cultural heritage that spans thousands of years.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "prompt=\"\"\"\n",
    "write a one line description of egypt\n",
    "\"\"\".strip()\n",
    "\n",
    "print(ollama(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Chain Response: The Northern Lights, also known as the Aurora Borealis, are a breathtaking natural phenomenon that has captivated humans for centuries. The Northern Lights are caused by a complex interaction between charged particles from the sun and the Earth's magnetic field.\n",
      "\n",
      "Here's a detailed explanation of the process:\n",
      "\n",
      "**The Sun's Magnetic Field**\n",
      "\n",
      "The solar wind is a stream of charged particles, mostly electrons and protons, emanating from the sun. These particles are accelerated to high speeds due to the sun's intense magnetic field. When the solar wind reaches the Earth's magnetic field, it interacts with the planet's magnetic poles.\n",
      "\n",
      "**Charged Particles and the Ionosphere**\n",
      "\n",
      "The solar wind is composed of charged particles, which include electrons, protons, and alpha particles (helium nuclei). These particles are attracted to the Earth's magnetic field and are funneled towards the magnetic poles. At the poles, the particles interact with the ionosphere, a region of the atmosphere where the air is ionized by solar radiation.\n",
      "\n",
      "**Excitation of Atmospheric Gases**\n",
      "\n",
      "When the solar wind particles collide with atmospheric gases such as oxygen and nitrogen in the ionosphere, they excite these atoms and molecules. This process causes them to gain energy and move to higher energy states. As a result, the atmosphere is heated, and the excited gas emits light at specific wavelengths.\n",
      "\n",
      "**Emission of Light**\n",
      "\n",
      "There are two types of light emission that occur during the Northern Lights:\n",
      "\n",
      "1. **Excitation-emission**: When the solar wind particles collide with atmospheric gases, they excite these atoms and molecules, causing them to emit light as they return to their ground state.\n",
      "2. **Bremsstrahlung**: When high-energy electrons collide with atomic nuclei in the atmosphere, they release energy in the form of X-rays. These X-rays are then converted into visible light by collisions with atmospheric gases.\n",
      "\n",
      "**Color of the Northern Lights**\n",
      "\n",
      "The color of the Northern Lights depends on the altitude at which the excited gas emits its light:\n",
      "\n",
      "* **Green**: Emitted at altitudes around 100-200 km, this is the most common color and results from the excitation of oxygen atoms.\n",
      "* **Red**: Emitted at higher altitudes (around 200-300 km), this color is produced by the excitation of oxygen molecules.\n",
      "* **Blue**: Less intense than green, this color is also emitted by excited oxygen atoms, but at slightly lower altitudes.\n",
      "\n",
      "**Other Factors that Influence the Northern Lights**\n",
      "\n",
      "While the solar wind and magnetic field are the primary causes of the Northern Lights, other factors can influence their appearance:\n",
      "\n",
      "* **Geomagnetic storms**: Changes in the Earth's magnetic field can cause more intense aurora displays.\n",
      "* **Solar activity**: The frequency and intensity of solar flares and coronal mass ejections (CMEs) can impact the strength of the Northern Lights.\n",
      "* **Atmospheric conditions**: Cloud cover, moon phase, and atmospheric temperature can affect the visibility and brightness of the Northern Lights.\n",
      "\n",
      "In summary, the Northern Lights are a spectacular display of charged particles from the sun interacting with the Earth's magnetic field and atmosphere. The resulting light emission is caused by the excitation and emission of atmospheric gases, producing a range of colors that can be influenced by various factors.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "# 1. Basic Template\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Please provide a detailed answer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=ollama, prompt=prompt)\n",
    "response = chain.invoke({\"question\": \"What causes northern lights?\"})\n",
    "\n",
    "# uncomment for prints\n",
    "# print(type(response))\n",
    "# print(response.keys())\n",
    "# print(response)\n",
    "print(\"Basic Chain Response:\", response['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is Paris the capital of France?\n",
      "Answer: True\n",
      "\n",
      "Question: Is the Earth flat?\n",
      "Answer: False\n",
      "\n",
      "Question: Is Python a programming language?\n",
      "Answer: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import BooleanOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "# Initialize the parser and LLM\n",
    "boolean_parser = BooleanOutputParser()\n",
    "\n",
    "ollama = Ollama(base_url='http://localhost:11434',\n",
    "                model=\"llama3.2\")\n",
    "\n",
    "# Create a template that will yield a yes/no answer\n",
    "template = \"\"\"Given the following question, answer with YES or NO only.\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template)\n",
    "\n",
    "# Create the chain\n",
    "# LCEL -> langchain expression language\n",
    "# https://python.langchain.com/v0.1/docs/expression_language/\n",
    "# Chain = prompt | LLM | Parser\n",
    "\n",
    "chain = prompt | ollama | boolean_parser\n",
    "\n",
    "# Example questions to test\n",
    "questions = [\n",
    "    \"Is Paris the capital of France?\",\n",
    "    \"Is the Earth flat?\",\n",
    "    \"Is Python a programming language?\"\n",
    "]\n",
    "\n",
    "# Run the chain for each question\n",
    "for question in questions:\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    # parsed_result = boolean_parser.parse(result['text'])\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result}\\n\")\n",
    "    # print(f\"parsed Answer: {parsed_result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: pyramid\n",
      "Explanation: ['Ancient Egyptian Construction Methods', 'Pyramid Architecture and Symbolism', \"Theories of Pyramids' Purpose and Functionality\"]\n",
      "\n",
      "Topic: egypt\n",
      "Explanation: ['Egyptian pyramids', 'mummification and burial practices', 'ancient Egyptian hieroglyphic writing system.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "commaseprated_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# generates topics\n",
    "topic_template = \"\"\"Generate three interesting subtopics about: {subject}\n",
    "Output only the subtopics, comma separated.\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "        input_variables=[\"subject\"],\n",
    "        template=topic_template\n",
    "    )\n",
    "\n",
    "topic_chain = prompt | ollama | commaseprated_parser\n",
    "\n",
    "# Run sequential chain\n",
    "subjects = {'egypt', 'pyramid'}\n",
    "\n",
    "for subject in subjects:\n",
    "    # Fix: Changed \"subjects\" to \"subject\" to match the template\n",
    "    Topics = topic_chain.invoke({\"subject\": subject})  \n",
    "    print(f\"\\nTopic: {subject}\")\n",
    "    print(f\"Topics: {Topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review Results:\n",
      "Title: The Matrix\n",
      "Rating: 9\n",
      "Reasons:\n",
      "- Innovative storyline\n",
      "- Impressive special effects\n",
      "- Thought-provoking themes\n",
      "- Strong performances\n",
      "- Visually stunning\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "class MovieReview(BaseModel):\n",
    "    title: str = Field(description=\"The title of the movie\")\n",
    "    rating: int = Field(description=\"Rating from 1-10\")\n",
    "    reasons: List[str] = Field(description=\"Reasons for the rating\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MovieReview)\n",
    "\n",
    "template = \"\"\"Create a movie review in JSON format for the movie: {movie}\n",
    "    \n",
    "    Your response must be a valid JSON object with these fields:\n",
    "    - title (string)\n",
    "    - rating (integer 1-10)\n",
    "    - reasons (array of strings)\n",
    "    \n",
    "    Respond with ONLY the JSON object.\"\"\"\n",
    "\n",
    "# Create prompt\n",
    "prompt = PromptTemplate(\n",
    "        input_variables=[\"movie\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "# full chain with parser\n",
    "chain = prompt | ollama | parser\n",
    "\n",
    "result = chain.invoke({\"movie\": \"The Matrix\"})\n",
    "print(\"\\nReview Results:\")\n",
    "print(f\"Title: {result.title}\")\n",
    "print(f\"Rating: {result.rating}\")\n",
    "print(\"Reasons:\")\n",
    "for reason in result.reasons:\n",
    "    print(f\"- {reason}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugging a memory system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Chat History: \n",
      "Human: What is machine learning?\n",
      "Assistant: Let me help you with that.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Chat History: Human: What is machine learning?\n",
      "AI: Machine learning is a type of artificial intelligence (AI) that enables computers to learn and improve their performance on a task without being explicitly programmed.\n",
      "\n",
      "In traditional programming, a computer is given a set of rules and instructions to follow. In contrast, machine learning allows the computer to discover patterns and relationships in data, and then make predictions or take actions based on that information.\n",
      "\n",
      "There are several types of machine learning, including:\n",
      "\n",
      "1. Supervised learning: This type of learning involves training a model on labeled data, where the correct output is already known. The goal is to learn the mapping between input data and output labels.\n",
      "2. Unsupervised learning: In this type of learning, the model is trained on unlabeled data, and it must find patterns or structure in the data on its own.\n",
      "3. Reinforcement learning: This type of learning involves training a model through trial and error, by receiving feedback in the form of rewards or penalties for its actions.\n",
      "\n",
      "Machine learning has many applications, including:\n",
      "\n",
      "1. Image recognition\n",
      "2. Natural language processing (NLP)\n",
      "3. Predictive analytics\n",
      "4. Autonomous vehicles\n",
      "\n",
      "The key benefits of machine learning include:\n",
      "\n",
      "1. Improved accuracy: Machine learning models can often achieve higher accuracy than traditional programming approaches.\n",
      "2. Increased efficiency: Machine learning models can automate many tasks, freeing up human time and resources.\n",
      "3. Scalability: Machine learning can handle large amounts of data and scale to meet the needs of growing organizations.\n",
      "\n",
      "However, machine learning also has its challenges and limitations, such as:\n",
      "\n",
      "1. Data quality: The accuracy of a machine learning model depends on the quality of the training data.\n",
      "2. Bias: Machine learning models can perpetuate biases in the training data if not designed carefully.\n",
      "3. Explainability: It can be difficult to understand how a machine learning model is making its predictions.\n",
      "\n",
      "I hope this helps! Do you have any specific questions about machine learning?\n",
      "Human: Can you give me some examples of its applications?\n",
      "Assistant: Let me help you with that.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Chat History: Human: What is machine learning?\n",
      "AI: Machine learning is a type of artificial intelligence (AI) that enables computers to learn and improve their performance on a task without being explicitly programmed.\n",
      "\n",
      "In traditional programming, a computer is given a set of rules and instructions to follow. In contrast, machine learning allows the computer to discover patterns and relationships in data, and then make predictions or take actions based on that information.\n",
      "\n",
      "There are several types of machine learning, including:\n",
      "\n",
      "1. Supervised learning: This type of learning involves training a model on labeled data, where the correct output is already known. The goal is to learn the mapping between input data and output labels.\n",
      "2. Unsupervised learning: In this type of learning, the model is trained on unlabeled data, and it must find patterns or structure in the data on its own.\n",
      "3. Reinforcement learning: This type of learning involves training a model through trial and error, by receiving feedback in the form of rewards or penalties for its actions.\n",
      "\n",
      "Machine learning has many applications, including:\n",
      "\n",
      "1. Image recognition\n",
      "2. Natural language processing (NLP)\n",
      "3. Predictive analytics\n",
      "4. Autonomous vehicles\n",
      "\n",
      "The key benefits of machine learning include:\n",
      "\n",
      "1. Improved accuracy: Machine learning models can often achieve higher accuracy than traditional programming approaches.\n",
      "2. Increased efficiency: Machine learning models can automate many tasks, freeing up human time and resources.\n",
      "3. Scalability: Machine learning can handle large amounts of data and scale to meet the needs of growing organizations.\n",
      "\n",
      "However, machine learning also has its challenges and limitations, such as:\n",
      "\n",
      "1. Data quality: The accuracy of a machine learning model depends on the quality of the training data.\n",
      "2. Bias: Machine learning models can perpetuate biases in the training data if not designed carefully.\n",
      "3. Explainability: It can be difficult to understand how a machine learning model is making its predictions.\n",
      "\n",
      "I hope this helps! Do you have any specific questions about machine learning?\n",
      "Human: Can you give me some examples of its applications?\n",
      "AI: Machine learning has numerous applications across various industries. Here are some examples:\n",
      "\n",
      "1. **Image Recognition**: Google's self-driving cars use machine learning to recognize and respond to visual data from cameras and sensors. Facebook uses machine learning to tag faces in photos.\n",
      "\n",
      "2. **Natural Language Processing (NLP)**: Virtual assistants like Siri, Alexa, and Google Assistant rely on machine learning to understand and generate human-like language.\n",
      "\n",
      "3. **Predictive Analytics**: Retailers use machine learning to predict customer behavior and make personalized recommendations. Airlines use machine learning to optimize flight routes and schedules.\n",
      "\n",
      "4. **Autonomous Vehicles**: Self-driving cars use machine learning to navigate roads, recognize obstacles, and make decisions in real-time.\n",
      "\n",
      "5. **Healthcare**: Machine learning is used to diagnose diseases, develop personalized treatment plans, and analyze medical images.\n",
      "\n",
      "6. **Customer Service Chatbots**: Many companies use machine learning-powered chatbots to provide 24/7 customer support and answer frequently asked questions.\n",
      "\n",
      "7. **Personalized Recommendations**: Netflix uses machine learning to recommend TV shows and movies based on users' viewing history.\n",
      "\n",
      "8. **Cybersecurity**: Machine learning is used to detect and prevent cyber attacks by analyzing network traffic patterns and identifying anomalies.\n",
      "\n",
      "9. **Finance**: Machine learning is used in stock trading, risk analysis, and credit scoring to predict market trends and make informed investment decisions.\n",
      "\n",
      "10. **Education**: Machine learning is used in adaptive learning systems to tailor educational content to individual students' needs and abilities.\n",
      "\n",
      "These are just a few examples of the many ways machine learning is being applied across industries.\n",
      "Human: What skills do I need to learn it?\n",
      "Assistant: Let me help you with that.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'human_input': 'What is machine learning?',\n",
       "  'chat_history': '',\n",
       "  'text': 'Machine learning is a type of artificial intelligence (AI) that enables computers to learn and improve their performance on a task without being explicitly programmed.\\n\\nIn traditional programming, a computer is given a set of rules and instructions to follow. In contrast, machine learning allows the computer to discover patterns and relationships in data, and then make predictions or take actions based on that information.\\n\\nThere are several types of machine learning, including:\\n\\n1. Supervised learning: This type of learning involves training a model on labeled data, where the correct output is already known. The goal is to learn the mapping between input data and output labels.\\n2. Unsupervised learning: In this type of learning, the model is trained on unlabeled data, and it must find patterns or structure in the data on its own.\\n3. Reinforcement learning: This type of learning involves training a model through trial and error, by receiving feedback in the form of rewards or penalties for its actions.\\n\\nMachine learning has many applications, including:\\n\\n1. Image recognition\\n2. Natural language processing (NLP)\\n3. Predictive analytics\\n4. Autonomous vehicles\\n\\nThe key benefits of machine learning include:\\n\\n1. Improved accuracy: Machine learning models can often achieve higher accuracy than traditional programming approaches.\\n2. Increased efficiency: Machine learning models can automate many tasks, freeing up human time and resources.\\n3. Scalability: Machine learning can handle large amounts of data and scale to meet the needs of growing organizations.\\n\\nHowever, machine learning also has its challenges and limitations, such as:\\n\\n1. Data quality: The accuracy of a machine learning model depends on the quality of the training data.\\n2. Bias: Machine learning models can perpetuate biases in the training data if not designed carefully.\\n3. Explainability: It can be difficult to understand how a machine learning model is making its predictions.\\n\\nI hope this helps! Do you have any specific questions about machine learning?'},\n",
       " {'human_input': 'Can you give me some examples of its applications?',\n",
       "  'chat_history': 'Human: What is machine learning?\\nAI: Machine learning is a type of artificial intelligence (AI) that enables computers to learn and improve their performance on a task without being explicitly programmed.\\n\\nIn traditional programming, a computer is given a set of rules and instructions to follow. In contrast, machine learning allows the computer to discover patterns and relationships in data, and then make predictions or take actions based on that information.\\n\\nThere are several types of machine learning, including:\\n\\n1. Supervised learning: This type of learning involves training a model on labeled data, where the correct output is already known. The goal is to learn the mapping between input data and output labels.\\n2. Unsupervised learning: In this type of learning, the model is trained on unlabeled data, and it must find patterns or structure in the data on its own.\\n3. Reinforcement learning: This type of learning involves training a model through trial and error, by receiving feedback in the form of rewards or penalties for its actions.\\n\\nMachine learning has many applications, including:\\n\\n1. Image recognition\\n2. Natural language processing (NLP)\\n3. Predictive analytics\\n4. Autonomous vehicles\\n\\nThe key benefits of machine learning include:\\n\\n1. Improved accuracy: Machine learning models can often achieve higher accuracy than traditional programming approaches.\\n2. Increased efficiency: Machine learning models can automate many tasks, freeing up human time and resources.\\n3. Scalability: Machine learning can handle large amounts of data and scale to meet the needs of growing organizations.\\n\\nHowever, machine learning also has its challenges and limitations, such as:\\n\\n1. Data quality: The accuracy of a machine learning model depends on the quality of the training data.\\n2. Bias: Machine learning models can perpetuate biases in the training data if not designed carefully.\\n3. Explainability: It can be difficult to understand how a machine learning model is making its predictions.\\n\\nI hope this helps! Do you have any specific questions about machine learning?',\n",
       "  'text': \"Machine learning has numerous applications across various industries. Here are some examples:\\n\\n1. **Image Recognition**: Google's self-driving cars use machine learning to recognize and respond to visual data from cameras and sensors. Facebook uses machine learning to tag faces in photos.\\n\\n2. **Natural Language Processing (NLP)**: Virtual assistants like Siri, Alexa, and Google Assistant rely on machine learning to understand and generate human-like language.\\n\\n3. **Predictive Analytics**: Retailers use machine learning to predict customer behavior and make personalized recommendations. Airlines use machine learning to optimize flight routes and schedules.\\n\\n4. **Autonomous Vehicles**: Self-driving cars use machine learning to navigate roads, recognize obstacles, and make decisions in real-time.\\n\\n5. **Healthcare**: Machine learning is used to diagnose diseases, develop personalized treatment plans, and analyze medical images.\\n\\n6. **Customer Service Chatbots**: Many companies use machine learning-powered chatbots to provide 24/7 customer support and answer frequently asked questions.\\n\\n7. **Personalized Recommendations**: Netflix uses machine learning to recommend TV shows and movies based on users' viewing history.\\n\\n8. **Cybersecurity**: Machine learning is used to detect and prevent cyber attacks by analyzing network traffic patterns and identifying anomalies.\\n\\n9. **Finance**: Machine learning is used in stock trading, risk analysis, and credit scoring to predict market trends and make informed investment decisions.\\n\\n10. **Education**: Machine learning is used in adaptive learning systems to tailor educational content to individual students' needs and abilities.\\n\\nThese are just a few examples of the many ways machine learning is being applied across industries.\"},\n",
       " {'human_input': 'What skills do I need to learn it?',\n",
       "  'chat_history': \"Human: What is machine learning?\\nAI: Machine learning is a type of artificial intelligence (AI) that enables computers to learn and improve their performance on a task without being explicitly programmed.\\n\\nIn traditional programming, a computer is given a set of rules and instructions to follow. In contrast, machine learning allows the computer to discover patterns and relationships in data, and then make predictions or take actions based on that information.\\n\\nThere are several types of machine learning, including:\\n\\n1. Supervised learning: This type of learning involves training a model on labeled data, where the correct output is already known. The goal is to learn the mapping between input data and output labels.\\n2. Unsupervised learning: In this type of learning, the model is trained on unlabeled data, and it must find patterns or structure in the data on its own.\\n3. Reinforcement learning: This type of learning involves training a model through trial and error, by receiving feedback in the form of rewards or penalties for its actions.\\n\\nMachine learning has many applications, including:\\n\\n1. Image recognition\\n2. Natural language processing (NLP)\\n3. Predictive analytics\\n4. Autonomous vehicles\\n\\nThe key benefits of machine learning include:\\n\\n1. Improved accuracy: Machine learning models can often achieve higher accuracy than traditional programming approaches.\\n2. Increased efficiency: Machine learning models can automate many tasks, freeing up human time and resources.\\n3. Scalability: Machine learning can handle large amounts of data and scale to meet the needs of growing organizations.\\n\\nHowever, machine learning also has its challenges and limitations, such as:\\n\\n1. Data quality: The accuracy of a machine learning model depends on the quality of the training data.\\n2. Bias: Machine learning models can perpetuate biases in the training data if not designed carefully.\\n3. Explainability: It can be difficult to understand how a machine learning model is making its predictions.\\n\\nI hope this helps! Do you have any specific questions about machine learning?\\nHuman: Can you give me some examples of its applications?\\nAI: Machine learning has numerous applications across various industries. Here are some examples:\\n\\n1. **Image Recognition**: Google's self-driving cars use machine learning to recognize and respond to visual data from cameras and sensors. Facebook uses machine learning to tag faces in photos.\\n\\n2. **Natural Language Processing (NLP)**: Virtual assistants like Siri, Alexa, and Google Assistant rely on machine learning to understand and generate human-like language.\\n\\n3. **Predictive Analytics**: Retailers use machine learning to predict customer behavior and make personalized recommendations. Airlines use machine learning to optimize flight routes and schedules.\\n\\n4. **Autonomous Vehicles**: Self-driving cars use machine learning to navigate roads, recognize obstacles, and make decisions in real-time.\\n\\n5. **Healthcare**: Machine learning is used to diagnose diseases, develop personalized treatment plans, and analyze medical images.\\n\\n6. **Customer Service Chatbots**: Many companies use machine learning-powered chatbots to provide 24/7 customer support and answer frequently asked questions.\\n\\n7. **Personalized Recommendations**: Netflix uses machine learning to recommend TV shows and movies based on users' viewing history.\\n\\n8. **Cybersecurity**: Machine learning is used to detect and prevent cyber attacks by analyzing network traffic patterns and identifying anomalies.\\n\\n9. **Finance**: Machine learning is used in stock trading, risk analysis, and credit scoring to predict market trends and make informed investment decisions.\\n\\n10. **Education**: Machine learning is used in adaptive learning systems to tailor educational content to individual students' needs and abilities.\\n\\nThese are just a few examples of the many ways machine learning is being applied across industries.\",\n",
       "  'text': 'To learn machine learning, you\\'ll want to develop a combination of technical and non-technical skills. Here are some key areas to focus on:\\n\\n**Technical Skills:**\\n\\n1. **Programming languages**: Python is a popular choice for machine learning, but knowledge of other languages like R, Java, or C++ can also be beneficial.\\n2. **Machine learning frameworks**: Familiarize yourself with popular libraries like TensorFlow, PyTorch, scikit-learn, or Keras.\\n3. **Data structures and algorithms**: Understand data preprocessing, feature engineering, and model selection.\\n4. **Statistics and probability**: Learn the basics of statistics, probability theory, and mathematical modeling.\\n\\n**Non-Technical Skills:**\\n\\n1. **Domain knowledge**: Familiarize yourself with a specific industry or domain, such as healthcare, finance, or marketing.\\n2. **Problem-solving**: Develop your analytical and problem-solving skills to tackle complex machine learning challenges.\\n3. **Communication**: Learn to explain technical concepts to non-technical stakeholders and collaborate effectively with teams.\\n\\n**Learning Resources:**\\n\\n1. Online courses:\\n\\t* Coursera\\'s Machine Learning specialization\\n\\t* edX\\'s Machine Learning course\\n\\t* Udemy\\'s Machine Learning tutorials\\n2. Books:\\n\\t* \"Python Machine Learning\" by Sebastian Raschka\\n\\t* \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\n3. Tutorials and blogs:\\n\\t* Kaggle\\'s tutorials and competitions\\n\\t* Machine Learning Mastery\\'s blog\\n\\n**Tips:**\\n\\n1. **Start with the basics**: Begin with introductory machine learning courses or tutorials to build a strong foundation.\\n2. **Practice consistently**: Work on personal projects or participate in machine learning competitions to apply your skills.\\n3. **Join online communities**: Engage with online forums, social media groups, and Reddit\\'s r/MachineLearning community to stay updated and learn from others.\\n\\nRemember, machine learning is a constantly evolving field. Stay curious, keep learning, and be patient with yourself as you develop your skills!'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "# Chat with Memory Example\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\" # is like naming our memory folder\n",
    ")\n",
    "\n",
    "chat_template = \"\"\"\n",
    "Chat History: {chat_history}\n",
    "Human: {human_input}\n",
    "Assistant: Let me help you with that.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    "    template=chat_template\n",
    ")\n",
    "\n",
    "chat_chain = LLMChain(\n",
    "    prompt=chat_prompt,\n",
    "    llm=ollama,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Simulate a conversation\n",
    "responses = [\n",
    "    chat_chain.invoke({\"human_input\": \"What is machine learning?\"}),\n",
    "    chat_chain.invoke({\"human_input\": \"Can you give me some examples of its applications?\"}),\n",
    "    chat_chain.invoke({\"human_input\": \"What skills do I need to learn it?\"})\n",
    "]\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/how_to/chatbots_memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \\\n",
    "#     langchain==0.3.26 \\\n",
    "#     langchain-core==0.3.66 \\\n",
    "#     langchain-community==0.3.26 \\\n",
    "#     langchain-ollama==0.3.3 \\\n",
    "#     langgraph==0.4.10 \\\n",
    "#     langchain-text-splitters==0.3.8 \\\n",
    "#     langsmith==0.4.2 \\\n",
    "#     ollama==0.5.1 \\\n",
    "#     pydantic==2.11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Invocation ---\n",
      "\n",
      "--- Final State after first call ---\n",
      "J'adore le programmement.\n",
      "\n",
      "(or)\n",
      "\n",
      "Je m'intresse beaucoup  l'informatique.\n",
      "\n",
      "(Note: \"programmer\" is more commonly used in English than in French, so the first option uses a loanword. The second option emphasizes the interest and use of computers.)\n",
      "\n",
      "--- Second Invocation (in the same thread) ---\n",
      "\n",
      "--- Final State after second call ---\n",
      "J'adore la construction d'intelligence artificielle.\n",
      "\n",
      "(Note: \"building AI\" can be translated to \"la construction d'intelligence artificielle\", but it's more common in English to use \"l'intelligence artificielle\" or simply \"l'IA\".)\n",
      "\n",
      " Alternative translation:\n",
      " J'ai une passion pour l'artificial intelligence.\n",
      "\n",
      "(This one emphasizes the enthusiasm and interest, rather than a literal \"building\" of AI.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import MessagesState\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"\n",
    "    The primary node that calls the LLM.\n",
    "    It takes the current list of messages and returns the AI's response.\n",
    "    \"\"\"\n",
    "    # The 'state' parameter is the entire current state of the graph\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # This is a robust way to add a system prompt only if one doesn't already exist\n",
    "    if not isinstance(messages[0], SystemMessage):\n",
    "        system_prompt = (\n",
    "            \"You are a helpful assistant. \"\n",
    "            \"Answer all questions to the best of your ability in a concise way.\"\n",
    "        )\n",
    "        messages = [SystemMessage(content=system_prompt)] + messages\n",
    "\n",
    "    # Invoke the model with the current messages\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # LangGraph will then automatically append this to the existing \"messages\" list in the state.\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Build and compile the graph ---\n",
    "\n",
    "# Initialize the graph and define the state shape\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add the 'call_model' function as a node in the graph\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Set the entry point of the graph. The first step after START is the 'model' node.\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer to remember conversation history\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "# Use the graph ---\n",
    "\n",
    "# The config dictionary is crucial for enabling memory.\n",
    "# 'thread_id' uniquely identifies this conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"french_translation_1\"}}\n",
    "\n",
    "# Invoke the app with the initial message\n",
    "print(\"--- Initial Invocation ---\")\n",
    "initial_input = {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]}\n",
    "response = app.invoke(initial_input, config=config)\n",
    "\n",
    "# Print the final state after the first invocation\n",
    "print(\"\\n--- Final State after first call ---\")\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "# You can continue the conversation, and it will remember the history\n",
    "print(\"\\n--- Second Invocation (in the same thread) ---\")\n",
    "follow_up_input = {\"messages\": [HumanMessage(content=\"What about 'I love building AI'?\")]}\n",
    "response = app.invoke(follow_up_input, config=config)\n",
    "\n",
    "print(\"\\n--- Final State after second call ---\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'   hello world   '\n",
      "17\n",
      "'hello world'\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Good practice is to clear the prompt with .strip\n",
    "# Without strip\n",
    "messy = \"   hello world   \"\n",
    "print(f\"'{messy}'\")\n",
    "print(len(messy))\n",
    "\n",
    "# With strip\n",
    "clean = messy.strip()\n",
    "print(f\"'{clean}'\")\n",
    "print(len(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert.\n",
      "    Your task is:\n",
      "    1. Analyze\n",
      "    2. Respond\n",
      "66\n",
      "You are an expert.\n",
      "Your task is:\n",
      "1. Analyze\n",
      "2. Respond\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "prompt1 = (\"\"\"\n",
    "    You are an expert.\n",
    "    Your task is:\n",
    "    1. Analyze\n",
    "    2. Respond\n",
    "\"\"\").strip()\n",
    "\n",
    "print(prompt1)\n",
    "print(len(prompt1))\n",
    "\n",
    "##############################################\n",
    "\n",
    "prompt2 = dedent(\"\"\"\n",
    "    You are an expert.\n",
    "    Your task is:\n",
    "    1. Analyze\n",
    "    2. Respond\n",
    "\"\"\").strip()\n",
    "\n",
    "print(prompt2)\n",
    "print(len(prompt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the .strip and dedunt affects anything ?? \n",
    "https://tiktokenizer.vercel.app/?model=gpt-4-1106-preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT PROMPTING ===\n",
      "Zero-shot SUCCESS:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Task: Classify this email as spam or legitimate\n",
      "Input: Get rich quick! Click here now!\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: I would classify this email as SPAM. The language used is overly promotional and suggests a get-rich-quick scheme, which is a common tactic used by spammers to lure people into investing in fake opportunities. The use of \"Click here now!\" also indicates a sense of urgency, which is often used to create a false sense of panic and prompt the recipient to act impulsively without verifying the authenticity of the email.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "print(\"=== ZERO-SHOT PROMPTING ===\")\n",
    "\n",
    "zero_shot_template = \"\"\"\n",
    "Task: {task}\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\".strip()\n",
    "\n",
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\", \"input_text\"],\n",
    "    template=zero_shot_template\n",
    ")\n",
    "\n",
    "zero_shot_chain = LLMChain(\n",
    "    prompt=zero_shot_prompt,\n",
    "    llm=ollama,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Success case\n",
    "print(\"Zero-shot SUCCESS:\")\n",
    "success_response = zero_shot_chain.invoke({\n",
    "    \"task\": \"Classify this email as spam or legitimate\",\n",
    "    \"input_text\": \"Get rich quick! Click here now!\"\n",
    "})\n",
    "print(f\"Response: {success_response['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Task: From the text, extract the person's name,\n",
      "                their job, and their two main tasks.\n",
      "                Format this as a nested JSON object.\n",
      "                The top-level key should be 'employee'. \n",
      "                This object should contain 'name', 'position',\n",
      "                and a 'duties' key which is a list of strings.\n",
      "Input: Dr. Eleanor Vance, the lead botanist,\n",
      "                is responsible for cataloging new plant species and monitoring\n",
      "                the greenhouse environment.\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: I don't see any text provided. Please provide the text about Dr. Eleanor Vance's job and tasks, and I'll be happy to help you extract the information in the requested format.\n"
     ]
    }
   ],
   "source": [
    "# Failure case\n",
    "complex_failure_response = zero_shot_chain.invoke({\n",
    "    \"task\": \"\"\"From the text, extract the person's name,\n",
    "                their job, and their two main tasks.\n",
    "                Format this as a nested JSON object.\n",
    "                The top-level key should be 'employee'. \n",
    "                This object should contain 'name', 'position',\n",
    "                and a 'duties' key which is a list of strings.\"\"\",\n",
    "\n",
    "    \"input_text\": \"\"\"Dr. Eleanor Vance, the lead botanist,\n",
    "                is responsible for cataloging new plant species and monitoring\n",
    "                the greenhouse environment.\"\"\"\n",
    "})\n",
    "print(f\"Response: {complex_failure_response['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONE-SHOT PROMPTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEW-SHOT PROMPTING ===\n",
      "Few-shot SUCCESS:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mFollow the example to extract the required information into the specified JSON format.\n",
      "---\n",
      "Example Output:\n",
      "{\n",
      "    \"employee\": {\n",
      "        \"name\": \"Mr. David Chen\",\n",
      "        \"position\": \"senior software architect\",\n",
      "        \"duties\": [\n",
      "            \"designs the system infrastructure\",\n",
      "            \"leads the code review process\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "---\n",
      "Now, complete the task for the following input.\n",
      "Input: Dr. Eleanor Vance, the lead botanist, is responsible for cataloging new plant species and monitoring the greenhouse environment.\n",
      "Output:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: {\n",
      "    \"employee\": {\n",
      "        \"name\": \"Dr. Eleanor Vance\",\n",
      "        \"position\": \"lead botanist\",\n",
      "        \"duties\": [\n",
      "            \"catalogs new plant species\",\n",
      "            \"monitors the greenhouse environment\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== FEW-SHOT PROMPTING ===\")\n",
    "\n",
    "few_shot_template = \"\"\"\n",
    "Follow the example to extract the required information into the specified JSON format.\n",
    "---\n",
    "Example Output:\n",
    "{{\n",
    "    \"employee\": {{\n",
    "        \"name\": \"Mr. David Chen\",\n",
    "        \"position\": \"senior software architect\",\n",
    "        \"duties\": [\n",
    "            \"designs the system infrastructure\",\n",
    "            \"leads the code review process\"\n",
    "        ]\n",
    "    }}\n",
    "}}\n",
    "---\n",
    "Now, complete the task for the following input.\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\".strip()\n",
    "\n",
    "# 2. Create a new PromptTemplate and Chain for the few-shot task\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=few_shot_template,\n",
    "    input_variables=[\"input_text\"]\n",
    ")\n",
    "\n",
    "few_shot_chain = LLMChain(\n",
    "    prompt=few_shot_prompt,\n",
    "    llm=ollama,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3. Invoke the chain with the same input that previously failed\n",
    "print(\"Few-shot SUCCESS:\")\n",
    "success_response = few_shot_chain.invoke({\n",
    "    \"input_text\": \"Dr. Eleanor Vance, the lead botanist, is responsible for cataloging new plant species and monitoring the greenhouse environment.\"\n",
    "})\n",
    "\n",
    "print(f\"Response: {success_response['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few Shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE: TICKET CLASSIFICATION ===\n",
      "Classifying a new, more complex support ticket:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a support ticket classifier. Analyze the user's message and classify it into one of the following categories: 'Billing Issue', 'Technical Support', or 'General Inquiry'.\n",
      "Assign a priority of 'High', 'Medium', or 'Low' based on the message's urgency. Provide the output in a JSON format.\n",
      "\n",
      "---\n",
      "Example 1\n",
      "Input: \"Hi, I can't log in to my dashboard. The password reset link you sent me is broken and I have an urgent deadline. Please help!\"\n",
      "Output:\n",
      "{\n",
      "    \"category\": \"Technical Support\",\n",
      "    \"priority\": \"High\"\n",
      "}\n",
      "---\n",
      "Example 2\n",
      "Input: \"I was looking at my last invoice and I believe I was overcharged. My plan is $25/month but I was billed for $50. Can you please correct this?\"\n",
      "Output:\n",
      "{\n",
      "    \"category\": \"Billing Issue\",\n",
      "    \"priority\": \"Medium\"\n",
      "}\n",
      "---\n",
      "Example 3\n",
      "Input: \"Hello, I was just wondering where I can find your company's privacy policy documentation on the website?\"\n",
      "Output:\n",
      "{\n",
      "    \"category\": \"General Inquiry\",\n",
      "    \"priority\": \"Low\"\n",
      "}\n",
      "---\n",
      "\n",
      "Now, complete the task for the following input.\n",
      "\n",
      "Input: My app keeps crashing every time I try to upload a file. It's not a complete outage, but it's really slowing down my work. What should I do?\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: {\n",
      "    \"category\": \"Technical Support\",\n",
      "    \"priority\": \"Medium\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EXAMPLE: TICKET CLASSIFICATION ===\")\n",
    "\n",
    "multi_shot_template = \"\"\"\n",
    "You are a support ticket classifier. Analyze the user's message and classify it into one of the following categories: 'Billing Issue', 'Technical Support', or 'General Inquiry'.\n",
    "Assign a priority of 'High', 'Medium', or 'Low' based on the message's urgency. Provide the output in a JSON format.\n",
    "\n",
    "---\n",
    "Example 1\n",
    "Input: \"Hi, I can't log in to my dashboard. The password reset link you sent me is broken and I have an urgent deadline. Please help!\"\n",
    "Output:\n",
    "{{\n",
    "    \"category\": \"Technical Support\",\n",
    "    \"priority\": \"High\"\n",
    "}}\n",
    "---\n",
    "Example 2\n",
    "Input: \"I was looking at my last invoice and I believe I was overcharged. My plan is $25/month but I was billed for $50. Can you please correct this?\"\n",
    "Output:\n",
    "{{\n",
    "    \"category\": \"Billing Issue\",\n",
    "    \"priority\": \"Medium\"\n",
    "}}\n",
    "---\n",
    "Example 3\n",
    "Input: \"Hello, I was just wondering where I can find your company's privacy policy documentation on the website?\"\n",
    "Output:\n",
    "{{\n",
    "    \"category\": \"General Inquiry\",\n",
    "    \"priority\": \"Low\"\n",
    "}}\n",
    "---\n",
    "\n",
    "Now, complete the task for the following input.\n",
    "\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create the PromptTemplate and Chain\n",
    "multi_shot_prompt = PromptTemplate(\n",
    "    template=multi_shot_template,\n",
    "    input_variables=[\"input_text\"]\n",
    ")\n",
    "\n",
    "multi_shot_chain = LLMChain(\n",
    "    prompt=multi_shot_prompt,\n",
    "    llm=ollama,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Classifying a new, more complex support ticket:\")\n",
    "final_ticket_response = multi_shot_chain.invoke({\n",
    "    \"input_text\": \"My app keeps crashing every time I try to upload a file. It's not a complete outage, but it's really slowing down my work. What should I do?\"\n",
    "})\n",
    "\n",
    "print(f\"Response: {final_ticket_response['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COT thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Planning: 1. Calculate the distance traveled at each speed \n",
      "2. Add those distances together\n",
      "3. Multiply by the number of times you traveled at that speed\n",
      "4. Add up all three results to get total distance\n",
      "\n",
      "Step 2 - Reasoning: Let's break down the problem step by step.\n",
      "\n",
      "Step 1: Calculate the distance traveled at each speed\n",
      "\n",
      "Distance = Speed x Time\n",
      "\n",
      "At 60 mph for 2.5 hours:\n",
      "Distance 1 = 60 x 2.5 = 150 miles\n",
      "\n",
      "At 80 mph for 1.5 hours:\n",
      "Distance 2 = 80 x 1.5 = 120 miles\n",
      "\n",
      "Step 2: Add those distances together\n",
      "Total distance = Distance 1 + Distance 2 = 150 + 120 = 270 miles\n",
      "\n",
      "Step 3: Multiply by the number of times you traveled at that speed\n",
      "\n",
      "You traveled each route twice, so multiply the total distance by 2:\n",
      "Total distance = 2 x 270 = 540 miles\n",
      "\n",
      "Step 4: Add up all three results to get total distance\n",
      "Since we've already calculated this in step 3, it's unnecessary to do it again. The result is simply 540 miles.\n",
      "\n",
      "Therefore, the total distance traveled by the train is 540 miles.\n",
      "\n",
      "Step 3 - Final Answer: The calculation provided for the total distance does not follow logically from the steps outlined in your explanation.\n",
      "\n",
      "\n",
      "To calculate the correct total distance:\n",
      "\n",
      "\n",
      "First, let's break down the problem step by step.\n",
      "\n",
      "Step 1: Calculate the distance traveled at each speed\n",
      "\n",
      "\n",
      "Distance = Speed x Time\n",
      "At 60 mph for 2.5 hours:\n",
      "Distance 1 = 60 * 2.5 = 150 miles\n",
      "\n",
      "At 80 mph for 1.5 hours:\n",
      "Distance 2 = 80 * 1.5 = 120 miles\n",
      "\n",
      "Step 2: Add those distances together\n",
      "To find the total distance, add Distance 1 and Distance 2:\n",
      "\n",
      "\n",
      "Total distance = Distance 1 + Distance 2 = 150 + 120 = 270 miles\n",
      "\n",
      "\n",
      "However, this is where Step 3 of your explanation comes in. According to your calculation method:\n",
      "You traveled each route twice, so multiply the total distance by 2:\n",
      "\n",
      "\n",
      "Total distance = 2 x 270 = 540 miles\n",
      "\n",
      "Therefore, the final clear answer is:\n",
      "\n",
      "The train actually traveled a total distance of 540 miles.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2:1b\" \n",
    ")\n",
    "\n",
    "# Step 1: Planning Chain\n",
    "planning_template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "What are the key steps I need to think about to solve this problem?\n",
    "List 3-4 main steps, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "planning_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=planning_template\n",
    ")\n",
    "\n",
    "planning_chain = LLMChain(llm=ollama, prompt=planning_prompt)\n",
    "\n",
    "# Step 2: Reasoning Chain  \n",
    "reasoning_template = \"\"\"\n",
    "Question: {question}\n",
    "Steps to follow: {steps}\n",
    "\n",
    "Now work through each step carefully and show your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"steps\"],\n",
    "    template=reasoning_template\n",
    ")\n",
    "\n",
    "reasoning_chain = LLMChain(llm=ollama, prompt=reasoning_prompt)\n",
    "\n",
    "# Step 3: Final Answer Chain\n",
    "answer_template = \"\"\"\n",
    "Question: {question}\n",
    "My reasoning: {reasoning}\n",
    "\n",
    "Based on my reasoning above, what is the final clear answer?\n",
    "\"\"\"\n",
    "\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"reasoning\"],\n",
    "    template=answer_template\n",
    ")\n",
    "\n",
    "answer_chain = LLMChain(llm=ollama, prompt=answer_prompt)\n",
    "\n",
    "# Run the multi-step chain\n",
    "question = \"If a train travels 60 mph for 2.5 hours, then 80 mph for 1.5 hours, what's the total distance?\"\n",
    "\n",
    "# Call 1: Get the steps\n",
    "steps_response = planning_chain.invoke({\"question\": question})\n",
    "print(\"Step 1 - Planning:\", steps_response['text'])\n",
    "\n",
    "# Call 2: Do the reasoning\n",
    "reasoning_response = reasoning_chain.invoke({\n",
    "    \"question\": question, \n",
    "    \"steps\": steps_response['text']\n",
    "})\n",
    "print(\"\\nStep 2 - Reasoning:\", reasoning_response['text'])\n",
    "\n",
    "# Call 3: Get final answer\n",
    "final_response = answer_chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"reasoning\": reasoning_response['text']\n",
    "})\n",
    "print(\"\\nStep 3 - Final Answer:\", final_response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT PROMPTING ===\n",
      "Zero-shot SUCCESS:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Task: Classify this email as spam or legitimate\n",
      "Input: Get rich quick! Click here now!\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: I would classify this email as SPAM. The language used is overly promotional and sensational, with the promise of \"getting rich quickly\" being a classic red flag for scams. Legitimate emails typically provide more information and context before making such bold claims.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama  \n",
    "from langchain_core.prompts import PromptTemplate  \n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "print(\"=== ZERO-SHOT PROMPTING ===\")\n",
    "\n",
    "zero_shot_template = \"\"\"\n",
    "Task: {task}\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\", \"input_text\"],\n",
    "    template=zero_shot_template\n",
    ")\n",
    "\n",
    "zero_shot_chain = LLMChain(\n",
    "    prompt=zero_shot_prompt,\n",
    "    llm=ollama,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Success case\n",
    "print(\"Zero-shot SUCCESS:\")\n",
    "success_response = zero_shot_chain.invoke({\n",
    "    \"task\": \"Classify this email as spam or legitimate\",\n",
    "    \"input_text\": \"Get rich quick! Click here now!\"\n",
    "})\n",
    "print(f\"Response: {success_response['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cache chat model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT PROMPTING ===\n",
      "Zero-shot SUCCESS:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Task: Classify this email as spam or legitimate\n",
      "Input: Get rich quick! Click here now!\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: I would classify this email as spam. The language used is sensational and attention-grabbing, with the promise of \"getting rich quickly\", which is often a tactic used by scammers to lure people into sending money or divulging personal information. There's no context or legitimacy provided in the email to suggest that it's a genuine opportunity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CORRECTED IMPORTS ---\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama  # Corrected import for community models\n",
    "from langchain.chains import LLMChain\n",
    "# ConversationBufferMemory is fine where it is\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\" \n",
    ")\n",
    "\n",
    "print(\"=== ZERO-SHOT PROMPTING ===\")\n",
    "\n",
    "zero_shot_template = \"\"\"\n",
    "Task: {task}\n",
    "Input: {input_text}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\", \"input_text\"],\n",
    "    template=zero_shot_template\n",
    ")\n",
    "\n",
    "zero_shot_chain = LLMChain(\n",
    "    prompt=zero_shot_prompt,\n",
    "    llm=ollama,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Success case\n",
    "print(\"Zero-shot SUCCESS:\")\n",
    "success_response = zero_shot_chain.invoke({\n",
    "    \"task\": \"Classify this email as spam or legitimate\",\n",
    "    \"input_text\": \"Get rich quick! Click here now!\"\n",
    "})\n",
    "print(f\"Response: {success_response['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First Call (should be slower) ---\n",
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta.\n",
      "Time taken: 3.41 seconds\n",
      "\n",
      "--- Second Call (should be much faster) ---\n",
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta.\n",
      "Time taken: 0.00 seconds\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# --- Define the LLM ---\n",
    "# This part was missing from the original snippet\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# --- Set up the cache ---\n",
    "# The key fix is importing set_llm_cache from langchain.globals\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# --- First invocation ---\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "print(\"--- First Call (should be slower) ---\")\n",
    "start_time = time.time()\n",
    "response1 = llm.invoke(\"Tell me a joke\")\n",
    "end_time = time.time()\n",
    "print(response1)\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\\n\")\n",
    "\n",
    "\n",
    "# --- Second invocation ---\n",
    "# The second time, the exact same input should be found in the cache\n",
    "# This call should be almost instantaneous\n",
    "print(\"--- Second Call (should be much faster) ---\")\n",
    "start_time = time.time()\n",
    "response2 = llm.invoke(\"Tell me a joke\")\n",
    "end_time = time.time()\n",
    "print(response2)\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Area calculation ===\n",
      "LLM wants to call: calculate_area(5, 3)\n",
      "Function result: The area is 15.0 square units\n",
      "Final Answer: To solve this problem, we can use the formula for the area of a rectangle:\n",
      "\n",
      "Area = Length x Width\n",
      "\n",
      "We are given that the length of the rectangle is 5 meters and the width is 3 meters. Plugging these values into the formula, we get:\n",
      "\n",
      "Area = 5 x 3\n",
      "= 15\n",
      "\n",
      "This matches the function result provided. Therefore, the area of the rectangle is indeed 15 square units.\n",
      "\n",
      "=== Test 2: Weather query ===\n",
      "LLM wants to call: get_weather(\"Cairo\")\n",
      "Function result: Hot, 95F\n",
      "Final Answer: I'm happy to help you with your question! However, I don't have real-time access to current weather conditions. But I can suggest some ways for you to find out the current weather in Cairo.\n",
      "\n",
      "You can check online weather websites or apps such as AccuWeather, Weather.com, or BBC Weather, which provide up-to-date weather forecasts and conditions for cities around the world, including Cairo.\n",
      "\n",
      "Alternatively, you can also check social media or news websites that often post updates on current weather conditions.\n",
      "\n",
      "That being said, based on historical climate data, I can tell you that Cairo is known to have a hot desert climate, with very little rainfall throughout the year. During the summer months, temperatures can reach as high as 95F (35C) and above, making it one of the hottest cities in the world.\n",
      "\n",
      "So while I don't have real-time information on the current weather in Cairo, I hope this gives you an idea of what to expect during different times of the year.\n",
      "\n",
      "=== Test 3: Regular question ===\n",
      "LLM wants to call: get_weather(\"Fairbanks\")\n",
      "Function result: Weather data not available\n",
      "Final Answer: The Northern Lights, also known as the Aurora Borealis, are a natural phenomenon that occurs when charged particles from the sun interact with the Earth's magnetic field and atmosphere. The main cause of the Northern Lights is the solar wind, which is a stream of charged particles emitted by the sun during solar flares and coronal mass ejections.\n",
      "\n",
      "Here's a step-by-step explanation:\n",
      "\n",
      "1. **Solar Wind**: The solar wind is a high-energy plasma that consists of electrons, protons, and alpha particles (helium nuclei). During intense solar activity, such as solar flares or coronal mass ejections, the solar wind increases in intensity.\n",
      "2. **Magnetic Field**: When the solar wind reaches the Earth's magnetic field, it is redirected towards the poles by the magnetic field lines. The magnetic field acts as a shield, deflecting the charged particles away from the equator and towards the poles.\n",
      "3. **Atmospheric Interaction**: At the poles, the solar wind particles collide with the Earth's atmosphere, causing the atoms and molecules in the air to become excited. This excitation leads to the emission of light at specific wavelengths, which we see as the Northern Lights.\n",
      "4. **Color and Movement**: The color of the Northern Lights depends on the energy of the particles and the altitude at which they collide with the atmosphere. Green is the most common color, produced by collisions at altitudes of around 100-200 km. Red is produced by collisions at higher altitudes, while blue and violet are produced by collisions at lower altitudes.\n",
      "5. **Aurora Season**: The Northern Lights are typically visible on clear, dark nights from late August to early April, when the Earth's tilt towards the sun results in more solar wind particles interacting with our magnetic field.\n",
      "\n",
      "In summary, the Northern Lights are caused by the interaction between the solar wind and the Earth's magnetic field, leading to the excitation of atmospheric atoms and molecules, which emit light as we see the aurora.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\"\n",
    ")\n",
    "\n",
    "# Simple function to call\n",
    "def calculate_area(length, width):\n",
    "    \"\"\"Calculate area of a rectangle\"\"\"\n",
    "    result = length * width\n",
    "    return f\"The area is {result} square units\"\n",
    "\n",
    "def get_weather(city):\n",
    "    \"\"\"Mock weather function\"\"\"\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 75F\",\n",
    "        \"London\": \"Rainy, 60F\", \n",
    "        \"Cairo\": \"Hot, 95F\"\n",
    "    }\n",
    "    return weather_data.get(city, \"Weather data not available\")\n",
    "\n",
    "# Function calling template\n",
    "function_template = \"\"\"\n",
    "You can call these functions:\n",
    "- calculate_area(length, width) - calculates rectangle area\n",
    "- get_weather(city) - gets weather for a city\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If you need to use a function, respond with:\n",
    "FUNCTION_CALL: function_name(param1, param2)\n",
    "\n",
    "Otherwise, just answer normally.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=function_template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=ollama, prompt=prompt)\n",
    "\n",
    "def process_response(question):\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    answer = response['text']\n",
    "    \n",
    "    # Check if LLM wants to call a function\n",
    "    if \"FUNCTION_CALL:\" in answer:\n",
    "        # Extract function call\n",
    "        match = re.search(r'FUNCTION_CALL:\\s*(\\w+)\\((.*?)\\)', answer)\n",
    "        if match:\n",
    "            func_name = match.group(1)\n",
    "            params = match.group(2)\n",
    "            \n",
    "            print(f\"LLM wants to call: {func_name}({params})\")\n",
    "            \n",
    "            # Execute the function\n",
    "            if func_name == \"calculate_area\":\n",
    "                # Parse parameters\n",
    "                nums = [float(x.strip()) for x in params.split(',')]\n",
    "                result = calculate_area(nums[0], nums[1])\n",
    "                \n",
    "            elif func_name == \"get_weather\":\n",
    "                city = params.strip().strip('\"\\'')\n",
    "                result = get_weather(city)\n",
    "            \n",
    "            print(f\"Function result: {result}\")\n",
    "            \n",
    "            # Get final response with function result\n",
    "            final_template = \"\"\"\n",
    "            Question: {question}\n",
    "            Function result: {function_result}\n",
    "            \n",
    "            Please provide a complete answer using this function result.\n",
    "            \"\"\"\n",
    "            \n",
    "            final_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"function_result\"],\n",
    "                template=final_template\n",
    "            )\n",
    "            \n",
    "            final_chain = LLMChain(llm=ollama, prompt=final_prompt)\n",
    "            final_response = final_chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"function_result\": result\n",
    "            })\n",
    "            \n",
    "            return final_response['text']\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the function calling\n",
    "print(\"=== Test 1: Area calculation ===\")\n",
    "result1 = process_response(\"What's the area of a rectangle that is 5 meters long and 3 meters wide?\")\n",
    "print(\"Final Answer:\", result1)\n",
    "\n",
    "print(\"\\n=== Test 2: Weather query ===\")\n",
    "result2 = process_response(\"What's the weather like in Cairo?\")\n",
    "print(\"Final Answer:\", result2)\n",
    "\n",
    "print(\"\\n=== Test 3: Regular question ===\")\n",
    "result3 = process_response(\"What causes northern lights?\")\n",
    "print(\"Final Answer:\", result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some personal data we want to chat with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= \"\"\"\n",
    "('Egypt is a treasure trove of breathtaking beauty, rich history, and vibrant '\n",
    " 'culture. Here are some of the most stunning aspects of Egypt:\\n'\n",
    " '\\n'\n",
    " '1. Pyramids of Giza: The last remaining ancient wonder of the Seven Wonders '\n",
    " 'of the Ancient World, the Pyramids of Giza are an awe-inspiring sight. The '\n",
    " 'largest pyramid, the Great Pyramid of Khufu, is an engineering marvel that '\n",
    " 'stands tall at over 481 feet (147 meters) high.\\n'\n",
    " '2. Nile River: The longest river in the world, the Nile flows through Egypt '\n",
    " 'and supports a rich ecosystem, including lush valleys, sandy deserts, and '\n",
    " 'majestic temples along its banks.\\n'\n",
    " '3. Valley of the Kings: Located on the west bank of the Nile, this vast '\n",
    " 'burial ground is home to over 60 pharaohs and their families, with elaborate '\n",
    " 'tombs carved into the limestone cliffs.\\n'\n",
    " '4. Temple of Karnak: One of the largest temple complexes in the world, the '\n",
    " 'Temple of Karnak is a stunning example of ancient Egyptian architecture, '\n",
    " 'featuring intricate hieroglyphics, obelisks, and grand halls.\\n'\n",
    " '5. Abu Simbel Temples: Located on the island of Philae in the Nile River, '\n",
    " 'these massive temples were carved out of solid rock to house the tombs of '\n",
    " 'Pharaoh Ramses II and his wife Nefertari.\\n'\n",
    " \"6. Red Sea Beaches: Egypt's coastline offers some of the most beautiful \"\n",
    " 'beaches in the world, with crystal-clear waters, white sand, and coral reefs '\n",
    " 'perfect for snorkeling and scuba diving.\\n'\n",
    " '7. Luxor Temple: This magnificent temple complex is one of the '\n",
    " 'best-preserved ancient structures in Egypt, featuring stunning reliefs, '\n",
    " 'columns, and a grand entrance.\\n'\n",
    " '8. Valley of the Queens: Located near Luxor, this burial ground is home to '\n",
    " 'over 100 tombs, including those of Pharaoh Hatshepsut and her sister '\n",
    " 'Nefertiti.\\n'\n",
    " \"9. Egyptian Museum: One of the world's greatest museums, the Egyptian Museum \"\n",
    " 'in Cairo houses an incredible collection of artifacts from ancient Egypt, '\n",
    " 'including mummies, sarcophagi, and other treasures.\\n'\n",
    " '10. Nile River Cruises: Take a relaxing boat ride along the Nile, exploring '\n",
    " 'the countryside, visiting temples, and enjoying the scenic views of the '\n",
    " 'river.\\n'\n",
    " '\\n'\n",
    " 'These are just a few of the many beautiful things to see in Egypt. With its '\n",
    " 'rich history, stunning landscapes, and vibrant culture, Egypt is a '\n",
    " 'destination that will leave you in awe.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=5)\n",
    "all_splits = text_splitter.split_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Egypt is a treasure trove of breathtaking\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding model\n",
    "\n",
    "ollama pull nomic-embed-text\n",
    "\n",
    "Pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_texts(texts=all_splits, embedding=oembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"what can you tell me about Pyramids of Giza?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"'of the Ancient World, the Pyramids of Giza are\"),\n",
       " Document(page_content=\"'1. Pyramids of Giza: The last remaining ancient\"),\n",
       " Document(page_content=\"'largest pyramid, the Great Pyramid of Khufu, is\"),\n",
       " Document(page_content=\"'best-preserved ancient structures in Egypt,\")]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
